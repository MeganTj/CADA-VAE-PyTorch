{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149ed80-b1e7-44d5-9f7c-d7249874fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..', '..'))\n",
    "# BabyARC-fewshot dataset for classification:\n",
    "from reasoning.experiments.concept_energy import get_dataset, ConceptDataset, ConceptFewshotDataset\n",
    "from reasoning.pytorch_net.util import init_args, plot_matrices, get_device\n",
    "from reasoning.util import visualize_matrices\n",
    "from reasoning.fsl_baselines.babyarc_eval_fewshot import load_model, get_babyarc_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649fb437-d615-42d7-8985-d75f41c3080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label(label, classes):\n",
    "    mapped_label = torch.LongTensor(label.size())\n",
    "    for i in range(classes.size(0)):\n",
    "        mapped_label[label==classes[i]] = i\n",
    "\n",
    "    return mapped_label\n",
    "\n",
    "\n",
    "EMBEDDING_DICT = {\n",
    "    \"c-Line->Eshape\": {\n",
    "        \"Line\": [\n",
    "            [1,0,0,0, 0,0,0, 0,0, 0,0],\n",
    "            [0,1,0,0, 0,0,0, 0,0, 0,0],\n",
    "            [0,0,1,0, 0,0,0, 0,0, 0,0],\n",
    "            [0,0,0,1, 0,0,0, 0,0, 0,0],\n",
    "        ],\n",
    "        \"Parallel\": [\n",
    "            [0,0,0,0, 1,0,0, 0,0, 0,0],\n",
    "            [0,0,0,0, 0,1,0, 0,0, 0,0],\n",
    "            [0,0,0,0, 0,0,1, 0,0, 0,0],\n",
    "        ],\n",
    "        \"VerticalMid\": [\n",
    "            [0,0,0,0, 0,0,0, 1,0, 0,0],\n",
    "            [0,0,0,0, 0,0,0, 0,1, 0,0],\n",
    "        ],\n",
    "        \"VerticalEdge\": [\n",
    "            [0,0,0,0, 0,0,0, 0,0, 1,0],\n",
    "            [0,0,0,0, 0,0,0, 0,0, 0,1],\n",
    "        ],\n",
    "        \"Eshape\": [\n",
    "            [1,1,1,1, 1,1,1, 1,0, 1,1],\n",
    "        ],\n",
    "        \"Fshape\": [\n",
    "            [1,1,1,0, 1,0,0, 1,0, 1,0],\n",
    "        ],\n",
    "        \"Ashape\": [\n",
    "            [1,1,1,1, 1,1,0, 1,1, 1,1],\n",
    "        ],\n",
    "    },\n",
    "    \"c-Eshape->RectE\": {\n",
    "        \"Eshape\": [\n",
    "            [1,  0,0,  0,0,0,0,  0,0,0,  0,0,0],\n",
    "        ],\n",
    "        \"Rect\": [\n",
    "            [0,  1,0,  0,0,0,0,  0,0,0,  0,0,0],\n",
    "            [0,  0,1,  0,0,0,0,  0,0,0,  0,0,0],\n",
    "        ],\n",
    "        \"IsNonOverlapXY\": [\n",
    "            [0,  0,0,  1,0,0,0,  0,0,0,  0,0,0],\n",
    "            [0,  0,0,  0,1,0,0,  0,0,0,  0,0,0],\n",
    "            [0,  0,0,  0,0,1,0,  0,0,0,  0,0,0],\n",
    "            [0,  0,0,  0,0,0,1,  0,0,0,  0,0,0],\n",
    "        ],\n",
    "        \"IsInside\": [\n",
    "            [0,  0,0,  0,0,0,0,  1,0,0,  0,0,0],\n",
    "            [0,  0,0,  0,0,0,0,  0,1,0,  0,0,0],\n",
    "            [0,  0,0,  0,0,0,0,  0,0,1,  0,0,0],\n",
    "        ],\n",
    "        \"IsEnclosed\": [\n",
    "            [0,  0,0,  0,0,0,0,  0,0,0,  1,0,0],\n",
    "            [0,  0,0,  0,0,0,0,  0,0,0,  0,1,0],\n",
    "            [0,  0,0,  0,0,0,0,  0,0,0,  0,0,1],\n",
    "        ],\n",
    "        \"RectE1a\": [\n",
    "            [1,  1,1,  1,1,1,1,  1,0,0,  1,0,0],\n",
    "        ],\n",
    "        \"RectE2a\": [\n",
    "            [1,  1,1,  1,1,0,0,  1,1,0,  1,1,0],\n",
    "        ],\n",
    "        \"RectE3a\": [\n",
    "            [1,  1,1,  0,0,0,0,  1,1,1,  1,1,1],\n",
    "        ],\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def get_label_embedding_from_c_label(c_label, mode):\n",
    "    if mode == \"c-Eshape->RectE\":\n",
    "        label_dict = {\n",
    "            \"Eshape\": [0], # 1\n",
    "            \"Rect\": [1,2], # 2,3\n",
    "            \"IsNonOverlapXY\": [3,4,5,6],\n",
    "            \"IsInside\": [7,8,9],\n",
    "            \"IsEnclosed\": [10,11,12],\n",
    "            \"RectE1a\": [13],\n",
    "            \"RectE2a\": [14],\n",
    "            \"RectE3a\": [15],\n",
    "        }\n",
    "    elif mode == \"c-Line->Eshape\":\n",
    "        label_dict = {\n",
    "            \"Line\": [0,1,2,3],\n",
    "            \"Parallel\": [4,5,6],\n",
    "            \"VerticalMid\": [7,8],\n",
    "            \"VerticalEdge\": [9,10],\n",
    "            \"Eshape\": [11],\n",
    "            \"Fshape\": [12],\n",
    "            \"Ashape\": [13],\n",
    "        }\n",
    "    else:\n",
    "        raise\n",
    "    c_label_cand = label_dict[c_label]\n",
    "    c_embedding_cand = EMBEDDING_DICT[mode][c_label]\n",
    "    idx = np.random.choice(len(c_label_cand))\n",
    "    c_label = c_label_cand[idx]\n",
    "    c_embedding = c_embedding_cand[idx]\n",
    "    return c_label, c_embedding\n",
    "\n",
    "\n",
    "LABEL_TO_C_LABEL = {\n",
    "    0: \"Line\",\n",
    "    1: \"Line\",\n",
    "    2: \"Line\",\n",
    "    3: \"Line\",\n",
    "    4: \"Parallel\",\n",
    "    5: \"Parallel\",\n",
    "    6: \"Parallel\",\n",
    "    7: \"VerticalMid\",\n",
    "    8: \"VerticalMid\",\n",
    "    9: \"VerticalEdge\",\n",
    "    10: \"VerticalEdge\",\n",
    "    11: \"Eshape\",\n",
    "    12: \"Fshape\",\n",
    "    13: \"Ashape\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734848c-1d24-4b44-a1b9-4f839cbd2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATA_LOADER(object):\n",
    "    def __init__(self, dataset, aux_datasource, device='cuda', is_full_info=False):\n",
    "\n",
    "        print(\"The current working directory is\")\n",
    "        print(os.getcwd())\n",
    "        folder = str(Path(os.getcwd()))\n",
    "        if folder[-5:] == 'model':\n",
    "            project_directory = Path(os.getcwd()).parent\n",
    "        else:\n",
    "            project_directory = folder\n",
    "\n",
    "        print('Project Directory:')\n",
    "        print(project_directory)\n",
    "        data_path = str(project_directory) + '/data'\n",
    "        print('Data Path')\n",
    "        print(data_path)\n",
    "        sys.path.append(data_path)\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.device = device\n",
    "        self.dataset = dataset\n",
    "        self.is_full_info = is_full_info\n",
    "        self.auxiliary_data_source = aux_datasource\n",
    "\n",
    "        self.all_data_sources = ['resnet_features'] + [self.auxiliary_data_source]\n",
    "\n",
    "        if self.dataset in ['c-Line->Eshape', 'c-Eshape->RectE']:\n",
    "            self.read_matdataset_concept(mode=self.dataset)\n",
    "        else:\n",
    "            if self.dataset == 'CUB':\n",
    "                self.datadir = self.data_path + '/CUB/'\n",
    "            elif self.dataset == 'SUN':\n",
    "                self.datadir = self.data_path + '/SUN/'\n",
    "            elif self.dataset == 'AWA1':\n",
    "                self.datadir = self.data_path + '/AWA1/'\n",
    "            elif self.dataset == 'AWA2':\n",
    "                self.datadir = self.data_path + '/AWA2/'\n",
    "            self.read_matdataset()\n",
    "\n",
    "        self.index_in_epoch = 0\n",
    "        self.epochs_completed = 0\n",
    "\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        #####################################################################\n",
    "        # gets batch from train_feature = 7057 samples from 150 train classes\n",
    "        #####################################################################\n",
    "        idx = torch.randperm(self.ntrain)[0:batch_size]\n",
    "        batch_feature = self.data['train_seen']['resnet_features'][idx]\n",
    "        batch_label =  self.data['train_seen']['labels'][idx]\n",
    "        batch_att = self.aux_data[batch_label]\n",
    "        return batch_label, [ batch_feature, batch_att]\n",
    "\n",
    "\n",
    "    def read_matdataset_concept(self, mode):\n",
    "        \"\"\"\n",
    "        For the concept dataset, during training, the seen classes are\n",
    "            \"Line\", \"Parallel\", \"VerticalMid\", \"VerticalEdge\". Their embeddings are\n",
    "            [Line,Line,Line,Line, Parallel,Parallel,Parallel, VerticalMid,VerticalMid, VerticalEdge,VerticalEdge]  # starts at 0,4,7,9\n",
    "            during training, if it is a Line e.g., one of the embedding will activate.\n",
    "\n",
    "        During inference, for a compositional concept, the embeddings corresponding to the concepts will be activated.\n",
    "            For example, for Eshape, it will be [1,1,1,1, 1,1,1, 1,0, 1,1]\n",
    "\n",
    "        \"\"\"\n",
    "        if mode == 'c-Line->Eshape':\n",
    "            concept_args = init_args({\n",
    "                \"dataset\": \"c-Line\",\n",
    "                \"seed\": 1,\n",
    "                \"n_examples\": 44000,\n",
    "                \"canvas_size\": 16,\n",
    "                \"rainbow_prob\": 0.,\n",
    "                \"color_avail\": \"1,2\",\n",
    "                \"w_type\": \"image+mask\",\n",
    "                \"max_n_distractors\": 2,\n",
    "                \"min_n_distractors\": 0,\n",
    "                \"allow_connect\": True,\n",
    "            })\n",
    "            concept_dataset, _ = get_dataset(concept_args, is_load=True)\n",
    "\n",
    "            relation_args = init_args({\n",
    "                \"dataset\": \"c-Parallel+VerticalMid+VerticalEdge\",\n",
    "                \"seed\": 1,\n",
    "                \"n_examples\": 44000,\n",
    "                \"canvas_size\": 16,\n",
    "                \"rainbow_prob\": 0.,\n",
    "                \"color_avail\": \"1,2\",\n",
    "                \"w_type\": \"image+mask\",\n",
    "                \"max_n_distractors\": 3,\n",
    "                \"min_n_distractors\": 0,\n",
    "                \"allow_connect\": True,\n",
    "            })\n",
    "            relation_dataset, _ = get_dataset(relation_args, is_load=True)\n",
    "\n",
    "            test_args = init_args({\n",
    "                \"dataset\": \"c-Eshape+Fshape+Ashape\",\n",
    "                \"seed\": 2,\n",
    "                \"n_examples\": 400,\n",
    "                \"canvas_size\": 16,\n",
    "                \"rainbow_prob\": 0.,\n",
    "                \"w_type\": \"image+mask\",\n",
    "                \"color_avail\": \"1,2\",\n",
    "                \"min_n_distractors\": 0,\n",
    "                \"max_n_distractors\": 0,\n",
    "                \"allow_connect\": True,\n",
    "                \"parsing_check\": False,\n",
    "            })\n",
    "            test_dataset, _ = get_dataset(test_args, is_load=True)\n",
    "        elif mode == 'c-Eshape->RectE':\n",
    "            concept_args = init_args({\n",
    "                \"dataset\": \"c-Rect[4,16]+Eshape[3,10]\",\n",
    "                \"seed\": 1,\n",
    "                \"n_examples\": 44000,\n",
    "                \"canvas_size\": 20,\n",
    "                \"rainbow_prob\": 0.,\n",
    "                \"w_type\": \"image+mask\",\n",
    "                \"color_avail\": \"1,2\",\n",
    "                \"max_n_distractors\": 2,\n",
    "                \"min_n_distractors\": 0,\n",
    "                \"allow_connect\": True,\n",
    "                \"parsing_check\": False,\n",
    "            })\n",
    "            concept_dataset, _ = get_dataset(concept_args, is_load=True)\n",
    "\n",
    "            relation_args = init_args({\n",
    "                \"dataset\": \"c-IsNonOverlapXY+IsInside+IsEnclosed(Rect[4,16]+Randshape[3,8]+Lshape[3,10]+Tshape[3,10])\",\n",
    "                \"seed\": 1,\n",
    "                \"n_examples\": 44000,\n",
    "                \"canvas_size\": 20,\n",
    "                \"rainbow_prob\": 0.,\n",
    "                \"w_type\": \"image+mask\",\n",
    "                \"color_avail\": \"1,2\",\n",
    "                \"max_n_distractors\": 1,\n",
    "                \"min_n_distractors\": 0,\n",
    "                \"allow_connect\": True,\n",
    "                \"parsing_check\": False,\n",
    "            })\n",
    "            relation_dataset, _ = get_dataset(relation_args, is_load=True)\n",
    "\n",
    "            test_args = init_args({\n",
    "                \"dataset\": \"c-RectE1a+RectE2a+RectE3a\",\n",
    "                \"seed\": 2,\n",
    "                \"n_examples\": 200,\n",
    "                \"canvas_size\": 20,\n",
    "                \"rainbow_prob\": 0.,\n",
    "                \"w_type\": \"image+mask\",\n",
    "                \"color_avail\": \"1,2\",\n",
    "                \"max_n_distractors\": 0,\n",
    "                \"min_n_distractors\": 0,\n",
    "                \"allow_connect\": True,\n",
    "                \"parsing_check\": False,\n",
    "            })\n",
    "            test_dataset, _ = get_dataset(test_args, is_load=True)\n",
    "            \n",
    "        else:\n",
    "            raise\n",
    "\n",
    "        train_img = []\n",
    "        train_mask = []\n",
    "        train_label = []\n",
    "        train_att = []\n",
    "\n",
    "        test_seen_img = []\n",
    "        test_seen_label = []\n",
    "\n",
    "        test_unseen_img = []\n",
    "        test_unseen_label = []\n",
    "        test_unseen_att = []\n",
    "\n",
    "        for data in concept_dataset:\n",
    "            img, masks, c_label, _ = data  # img: [10,16,16]\n",
    "            label, c_embedding = get_label_embedding_from_c_label(c_label, mode=mode)\n",
    "            train_img.append(img)\n",
    "            train_label.append(label)\n",
    "            train_att.append(c_embedding)\n",
    "            train_mask.append(torch.cat([torch.cat(masks), torch.zeros(masks[0].shape)]))\n",
    "\n",
    "        for data in relation_dataset:\n",
    "            img, masks, c_label, _ = data  # img: [10,16,16]\n",
    "            label, c_embedding = get_label_embedding_from_c_label(c_label, mode=mode)\n",
    "            train_img.append(img)\n",
    "            train_label.append(label)\n",
    "            train_att.append(c_embedding)\n",
    "            train_mask.append(torch.cat(masks))\n",
    "\n",
    "        for data in test_dataset:\n",
    "            img, _, c_label, _ = data  # img: [10,16,16]\n",
    "            label, c_embedding = get_label_embedding_from_c_label(c_label, mode=mode)\n",
    "            test_unseen_img.append(img)\n",
    "            test_unseen_label.append(label)\n",
    "            test_unseen_att.append(c_embedding)\n",
    "\n",
    "        train_img = torch.stack(train_img).to(\"cpu\")  # [88000, 10, 16, 16]\n",
    "        train_label = torch.LongTensor(train_label).to(self.device)  # [88000]\n",
    "        train_att = torch.FloatTensor(train_att).to(self.device)  # [88000, 11]\n",
    "        train_mask = torch.stack(train_mask).to(self.device)\n",
    "\n",
    "        test_unseen_img = torch.stack(test_unseen_img).to(\"cpu\")  # [400, 10, 16, 16]\n",
    "        test_unseen_label = torch.LongTensor(test_unseen_label).to(self.device)  # [400]\n",
    "        test_unseen_att = torch.FloatTensor(test_unseen_att).to(self.device)  # [400, 11]\n",
    "\n",
    "        List = []\n",
    "        for key, item in EMBEDDING_DICT[mode].items():\n",
    "            List += item\n",
    "        self.aux_data = torch.FloatTensor(List).to(self.device)\n",
    "        \n",
    "        if mode == 'c-Line->Eshape':\n",
    "            model_args = init_args({\n",
    "                'model': 'resnet12_ssl',\n",
    "                'model_path': '/dfs/user/tailin/.results/fsl_baselines/backup/babyarc_resnet12_ssl_ground_lr_0.005_decay_0.0005_trans_2d_trial_1/model_cosmic-water-212.pth',\n",
    "                'n_deconv_conv': 0,\n",
    "                'lst_channels': [64, 160, 320, 640],\n",
    "                'is_3d': False,\n",
    "                'use_easy_aug': False,\n",
    "                'task': 'ground',\n",
    "                'training_ver': '',\n",
    "                'fs': 'complex_v2',\n",
    "                'data_root': '/raid/data/IncrementLearn/imagenet/Datasets/MiniImagenet/',\n",
    "                'simclr': False,\n",
    "                'n_aug_support_samples': 5,\n",
    "                'num_workers': 3,\n",
    "                'test_batch_size': 1,\n",
    "                'batch_size': 64,\n",
    "                'ft_batch_size': 1,\n",
    "                'ft_epochs': 10,\n",
    "                'ft_learning_rate': 0.02,\n",
    "                'ft_weight_decay': 0.0005,\n",
    "                'ft_momentum': 0.9,\n",
    "                'ft_adam': False,\n",
    "                'data_aug': True,\n",
    "                'n_cls': 7\n",
    "            })\n",
    "        elif mode == 'c-Eshape->RectE':\n",
    "            model_args = init_args({\n",
    "                'model': 'resnet12_ssl',\n",
    "                'model_path': '/dfs/user/tailin/.results/fsl_baselines/backup/babyarc_resnet12_ssl_ground_lr_0.005_decay_0.0005_trans_2d_trial_1/model_fresh-flower-42.pth',\n",
    "                'n_deconv_conv': 0,\n",
    "                'lst_channels': [64, 160, 320, 640],\n",
    "                'is_3d': False,\n",
    "                'use_easy_aug': False,\n",
    "                'task': 'ground',\n",
    "                'training_ver': '',\n",
    "                'fs': 'complex_v2',\n",
    "                'data_root': '/raid/data/IncrementLearn/imagenet/Datasets/MiniImagenet/',\n",
    "                'simclr': False,\n",
    "                'n_aug_support_samples': 5,\n",
    "                'num_workers': 3,\n",
    "                'test_batch_size': 1,\n",
    "                'batch_size': 64,\n",
    "                'ft_batch_size': 1,\n",
    "                'ft_epochs': 10,\n",
    "                'ft_learning_rate': 0.02,\n",
    "                'ft_weight_decay': 0.0005,\n",
    "                'ft_momentum': 0.9,\n",
    "                'ft_adam': False,\n",
    "                'data_aug': True,\n",
    "                'n_cls': 7\n",
    "            })\n",
    "        else:\n",
    "            raise\n",
    "        resnet_model = load_model(model_args).to(\"cpu\")\n",
    "        train_feature = resnet_model.encode(train_img).detach().to(self.device)\n",
    "        test_seen_feature = []\n",
    "        test_unseen_feature = resnet_model.encode(test_unseen_img).detach().to(self.device)\n",
    "\n",
    "        self.seenclasses = torch.from_numpy(np.unique(train_label.cpu().numpy())).to(self.device) # [40]\n",
    "        self.novelclasses = torch.from_numpy(np.unique(test_unseen_label.cpu().numpy())).to(self.device)  # [10]\n",
    "        self.ntrain = train_feature.size()[0]  # 19832\n",
    "        self.ntrain_class = self.seenclasses.size(0)  # 40\n",
    "        self.ntest_class = self.novelclasses.size(0)  # 10\n",
    "        self.train_class = self.seenclasses.clone()  # [40]\n",
    "        self.allclasses = torch.arange(0, self.ntrain_class+self.ntest_class).long()  #[0,...49]\n",
    "\n",
    "        self.train_mapped_label = map_label(train_label, self.seenclasses)  # [19832]\n",
    "\n",
    "        self.data = {}\n",
    "        self.data['train_seen'] = {}\n",
    "        self.data['train_seen']['resnet_features'] = train_feature\n",
    "        self.data['train_seen']['labels']= train_label\n",
    "        self.data['train_seen'][self.auxiliary_data_source] = self.aux_data[train_label]   # [19832, 85]\n",
    "        if self.is_full_info:\n",
    "            self.data['train_seen']['masks'] = train_mask\n",
    "            self.data['train_seen']['imgs'] = train_img\n",
    "\n",
    "        self.data['train_unseen'] = {}\n",
    "        self.data['train_unseen']['resnet_features'] = None\n",
    "        self.data['train_unseen']['labels'] = None\n",
    "\n",
    "        self.data['test_seen'] = {}\n",
    "        self.data['test_seen']['resnet_features'] = test_seen_feature  # [4958, 2048]\n",
    "        self.data['test_seen']['labels'] = test_seen_label\n",
    "\n",
    "        self.data['test_unseen'] = {}\n",
    "        self.data['test_unseen']['resnet_features'] = test_unseen_feature\n",
    "        self.data['test_unseen'][self.auxiliary_data_source] = self.aux_data[test_unseen_label]  # [5685, 85]\n",
    "        self.data['test_unseen']['labels'] = test_unseen_label  # [5685]\n",
    "        self.data['test_unseen']['imgs'] = test_unseen_img\n",
    "\n",
    "        self.novelclass_aux_data = self.aux_data[self.novelclasses]  # [3, 11]\n",
    "        self.seenclass_aux_data = self.aux_data[self.seenclasses] # [11, 11]\n",
    "\n",
    "\n",
    "    def read_matdataset(self):\n",
    "\n",
    "        path= self.datadir + 'res101.mat'\n",
    "        print('_____')\n",
    "        print(path)\n",
    "        matcontent = sio.loadmat(path) # keys: 'image_files', 'features', 'labels']\n",
    "        feature = matcontent['features'].T  # [30475, 2048]\n",
    "        label = matcontent['labels'].astype(int).squeeze() - 1  # [30475]\n",
    "\n",
    "        path= self.datadir + 'att_splits.mat'\n",
    "        matcontent = sio.loadmat(path)\n",
    "        # numpy array index starts from 0, matlab starts from 1\n",
    "        trainval_loc = matcontent['trainval_loc'].squeeze() - 1  # (19832,)\n",
    "        train_loc = matcontent['train_loc'].squeeze() - 1 # (16864,) --> train_feature = TRAIN SEEN\n",
    "        val_unseen_loc = matcontent['val_loc'].squeeze() - 1 #(7926,)--> test_unseen_feature = TEST UNSEEN\n",
    "        test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1  # (4958,)\n",
    "        test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1  # (5685,)\n",
    "\n",
    "\n",
    "        if self.auxiliary_data_source == 'attributes':\n",
    "            self.aux_data = torch.from_numpy(matcontent['att'].T).float().to(self.device)  # [50, 85]\n",
    "        else:\n",
    "            if self.dataset != 'CUB':\n",
    "                print('the specified auxiliary datasource is not available for this dataset')\n",
    "            else:\n",
    "\n",
    "                with open(self.datadir + 'CUB_supporting_data.p', 'rb') as h:\n",
    "                    x = pickle.load(h)\n",
    "                    self.aux_data = torch.from_numpy(x[self.auxiliary_data_source]).float().to(self.device)\n",
    "\n",
    "\n",
    "                print('loaded ', self.auxiliary_data_source)\n",
    "\n",
    "\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        train_feature = scaler.fit_transform(feature[trainval_loc])  # (19832, 2048)\n",
    "        test_seen_feature = scaler.transform(feature[test_seen_loc])  # (4958, 2048)\n",
    "        test_unseen_feature = scaler.transform(feature[test_unseen_loc])  # (5685, 2048)\n",
    "\n",
    "        train_feature = torch.from_numpy(train_feature).float().to(self.device)\n",
    "        test_seen_feature = torch.from_numpy(test_seen_feature).float().to(self.device)\n",
    "        test_unseen_feature = torch.from_numpy(test_unseen_feature).float().to(self.device)\n",
    "\n",
    "        train_label = torch.from_numpy(label[trainval_loc]).long().to(self.device)\n",
    "        test_unseen_label = torch.from_numpy(label[test_unseen_loc]).long().to(self.device)\n",
    "        test_seen_label = torch.from_numpy(label[test_seen_loc]).long().to(self.device)\n",
    "\n",
    "        self.seenclasses = torch.from_numpy(np.unique(train_label.cpu().numpy())).to(self.device) # [40]\n",
    "        self.novelclasses = torch.from_numpy(np.unique(test_unseen_label.cpu().numpy())).to(self.device)  # [10]\n",
    "        self.ntrain = train_feature.size()[0]  # 19832\n",
    "        self.ntrain_class = self.seenclasses.size(0)  # 40\n",
    "        self.ntest_class = self.novelclasses.size(0)  # 10\n",
    "        self.train_class = self.seenclasses.clone()  # [40]\n",
    "        self.allclasses = torch.arange(0, self.ntrain_class+self.ntest_class).long()  #[0,...49]\n",
    "\n",
    "        self.train_mapped_label = map_label(train_label, self.seenclasses)  # [19832]\n",
    "\n",
    "        self.data = {}\n",
    "        self.data['train_seen'] = {}\n",
    "        self.data['train_seen']['resnet_features'] = train_feature\n",
    "        self.data['train_seen']['labels']= train_label\n",
    "        self.data['train_seen'][self.auxiliary_data_source] = self.aux_data[train_label]  # [19832, 85]\n",
    "\n",
    "\n",
    "        self.data['train_unseen'] = {}\n",
    "        self.data['train_unseen']['resnet_features'] = None\n",
    "        self.data['train_unseen']['labels'] = None\n",
    "\n",
    "        self.data['test_seen'] = {}\n",
    "        self.data['test_seen']['resnet_features'] = test_seen_feature\n",
    "        self.data['test_seen']['labels'] = test_seen_label\n",
    "\n",
    "        self.data['test_unseen'] = {}\n",
    "        self.data['test_unseen']['resnet_features'] = test_unseen_feature\n",
    "        self.data['test_unseen'][self.auxiliary_data_source] = self.aux_data[test_unseen_label]\n",
    "        self.data['test_unseen']['labels'] = test_unseen_label\n",
    "\n",
    "        self.novelclass_aux_data = self.aux_data[self.novelclasses]\n",
    "        self.seenclass_aux_data = self.aux_data[self.seenclasses]\n",
    "\n",
    "\n",
    "    def transfer_features(self, n, num_queries='num_features'):\n",
    "        \"\"\"Only used for few-shot learning.\"\"\"\n",
    "        print('size before')\n",
    "        print(self.data['test_unseen']['resnet_features'].size())\n",
    "        print(self.data['train_seen']['resnet_features'].size())\n",
    "\n",
    "\n",
    "        print('o'*100)\n",
    "        print(self.data['test_unseen'].keys())\n",
    "        for i,s in enumerate(self.novelclasses):\n",
    "\n",
    "            features_of_that_class   = self.data['test_unseen']['resnet_features'][self.data['test_unseen']['labels']==s ,:]\n",
    "\n",
    "            if 'attributes' == self.auxiliary_data_source:\n",
    "                attributes_of_that_class = self.data['test_unseen']['attributes'][self.data['test_unseen']['labels']==s ,:]\n",
    "                use_att = True\n",
    "            else:\n",
    "                use_att = False\n",
    "            if 'sentences' == self.auxiliary_data_source:\n",
    "                sentences_of_that_class = self.data['test_unseen']['sentences'][self.data['test_unseen']['labels']==s ,:]\n",
    "                use_stc = True\n",
    "            else:\n",
    "                use_stc = False\n",
    "            if 'word2vec' == self.auxiliary_data_source:\n",
    "                word2vec_of_that_class = self.data['test_unseen']['word2vec'][self.data['test_unseen']['labels']==s ,:]\n",
    "                use_w2v = True\n",
    "            else:\n",
    "                use_w2v = False\n",
    "            if 'glove' == self.auxiliary_data_source:\n",
    "                glove_of_that_class = self.data['test_unseen']['glove'][self.data['test_unseen']['labels']==s ,:]\n",
    "                use_glo = True\n",
    "            else:\n",
    "                use_glo = False\n",
    "            if 'wordnet' == self.auxiliary_data_source:\n",
    "                wordnet_of_that_class = self.data['test_unseen']['wordnet'][self.data['test_unseen']['labels']==s ,:]\n",
    "                use_hie = True\n",
    "            else:\n",
    "                use_hie = False\n",
    "\n",
    "\n",
    "            num_features = features_of_that_class.size(0)\n",
    "\n",
    "            indices = torch.randperm(num_features)\n",
    "\n",
    "            if num_queries!='num_features':\n",
    "\n",
    "                indices = indices[:n+num_queries]\n",
    "\n",
    "\n",
    "            print(features_of_that_class.size())\n",
    "\n",
    "\n",
    "            if i==0:\n",
    "\n",
    "                new_train_unseen      = features_of_that_class[   indices[:n] ,:]\n",
    "\n",
    "                if use_att:\n",
    "                    new_train_unseen_att  = attributes_of_that_class[ indices[:n] ,:]\n",
    "                if use_stc:\n",
    "                    new_train_unseen_stc  = sentences_of_that_class[ indices[:n] ,:]\n",
    "                if use_w2v:\n",
    "                    new_train_unseen_w2v  = word2vec_of_that_class[ indices[:n] ,:]\n",
    "                if use_glo:\n",
    "                    new_train_unseen_glo  = glove_of_that_class[ indices[:n] ,:]\n",
    "                if use_hie:\n",
    "                    new_train_unseen_hie  = wordnet_of_that_class[ indices[:n] ,:]\n",
    "\n",
    "\n",
    "                new_train_unseen_label  = s.repeat(n)\n",
    "\n",
    "                new_test_unseen = features_of_that_class[  indices[n:] ,:]\n",
    "\n",
    "                new_test_unseen_label = s.repeat( len(indices[n:] ))\n",
    "\n",
    "            else:\n",
    "                new_train_unseen  = torch.cat(( new_train_unseen             , features_of_that_class[  indices[:n] ,:]),dim=0)\n",
    "                new_train_unseen_label  = torch.cat(( new_train_unseen_label , s.repeat(n)),dim=0)\n",
    "\n",
    "                new_test_unseen =  torch.cat(( new_test_unseen,    features_of_that_class[  indices[n:] ,:]),dim=0)\n",
    "                new_test_unseen_label = torch.cat(( new_test_unseen_label  ,s.repeat( len(indices[n:]) )) ,dim=0)\n",
    "\n",
    "                if use_att:\n",
    "                    new_train_unseen_att    = torch.cat(( new_train_unseen_att   , attributes_of_that_class[indices[:n] ,:]),dim=0)\n",
    "                if use_stc:\n",
    "                    new_train_unseen_stc    = torch.cat(( new_train_unseen_stc   , sentences_of_that_class[indices[:n] ,:]),dim=0)\n",
    "                if use_w2v:\n",
    "                    new_train_unseen_w2v    = torch.cat(( new_train_unseen_w2v   , word2vec_of_that_class[indices[:n] ,:]),dim=0)\n",
    "                if use_glo:\n",
    "                    new_train_unseen_glo    = torch.cat(( new_train_unseen_glo   , glove_of_that_class[indices[:n] ,:]),dim=0)\n",
    "                if use_hie:\n",
    "                    new_train_unseen_hie    = torch.cat(( new_train_unseen_hie   , wordnet_of_that_class[indices[:n] ,:]),dim=0)\n",
    "\n",
    "\n",
    "\n",
    "        print('new_test_unseen.size(): ', new_test_unseen.size())\n",
    "        print('new_test_unseen_label.size(): ', new_test_unseen_label.size())\n",
    "        print('new_train_unseen.size(): ', new_train_unseen.size())\n",
    "        #print('new_train_unseen_att.size(): ', new_train_unseen_att.size())\n",
    "        print('new_train_unseen_label.size(): ', new_train_unseen_label.size())\n",
    "        print('>> num novel classes: ' + str(len(self.novelclasses)))\n",
    "\n",
    "        #######\n",
    "        ##\n",
    "        #######\n",
    "\n",
    "        self.data['test_unseen']['resnet_features'] = copy.deepcopy(new_test_unseen)\n",
    "        #self.data['train_seen']['resnet_features']  = copy.deepcopy(new_train_seen)\n",
    "\n",
    "        self.data['test_unseen']['labels'] = copy.deepcopy(new_test_unseen_label)\n",
    "        #self.data['train_seen']['labels']  = copy.deepcopy(new_train_seen_label)\n",
    "\n",
    "        self.data['train_unseen']['resnet_features'] = copy.deepcopy(new_train_unseen)\n",
    "        self.data['train_unseen']['labels'] = copy.deepcopy(new_train_unseen_label)\n",
    "        self.ntrain_unseen = self.data['train_unseen']['resnet_features'].size(0)\n",
    "\n",
    "        if use_att:\n",
    "            self.data['train_unseen']['attributes'] = copy.deepcopy(new_train_unseen_att)\n",
    "        if use_w2v:\n",
    "            self.data['train_unseen']['word2vec']   = copy.deepcopy(new_train_unseen_w2v)\n",
    "        if use_stc:\n",
    "            self.data['train_unseen']['sentences']  = copy.deepcopy(new_train_unseen_stc)\n",
    "        if use_glo:\n",
    "            self.data['train_unseen']['glove']      = copy.deepcopy(new_train_unseen_glo)\n",
    "        if use_hie:\n",
    "            self.data['train_unseen']['wordnet']   = copy.deepcopy(new_train_unseen_hie)\n",
    "\n",
    "        ####\n",
    "        self.data['train_seen_unseen_mixed'] = {}\n",
    "        self.data['train_seen_unseen_mixed']['resnet_features'] = torch.cat((self.data['train_seen']['resnet_features'],self.data['train_unseen']['resnet_features']),dim=0)\n",
    "        self.data['train_seen_unseen_mixed']['labels'] = torch.cat((self.data['train_seen']['labels'],self.data['train_unseen']['labels']),dim=0)\n",
    "\n",
    "        self.ntrain_mixed = self.data['train_seen_unseen_mixed']['resnet_features'].size(0)\n",
    "\n",
    "        if use_att:\n",
    "            self.data['train_seen_unseen_mixed']['attributes'] = torch.cat((self.data['train_seen']['attributes'],self.data['train_unseen']['attributes']),dim=0)\n",
    "        if use_w2v:\n",
    "            self.data['train_seen_unseen_mixed']['word2vec'] = torch.cat((self.data['train_seen']['word2vec'],self.data['train_unseen']['word2vec']),dim=0)\n",
    "        if use_stc:\n",
    "            self.data['train_seen_unseen_mixed']['sentences'] = torch.cat((self.data['train_seen']['sentences'],self.data['train_unseen']['sentences']),dim=0)\n",
    "        if use_glo:\n",
    "            self.data['train_seen_unseen_mixed']['glove'] = torch.cat((self.data['train_seen']['glove'],self.data['train_unseen']['glove']),dim=0)\n",
    "        if use_hie:\n",
    "            self.data['train_seen_unseen_mixed']['wordnet'] = torch.cat((self.data['train_seen']['wordnet'],self.data['train_unseen']['wordnet']),dim=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097a27a-12c3-4b76-a658-690f254e1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    d = DATA_LOADER(dataset=\"c-Line->Eshape\", aux_datasource=\"attributes\", is_full_info=True)\n",
    "    \n",
    "    for i in range(44000,44040):\n",
    "        img = d.data['train_seen']['imgs'][i]\n",
    "        masks = d.data['train_seen']['masks'][i]\n",
    "        attr = d.data['train_seen']['attributes'][i]\n",
    "        label = d.data['train_seen']['labels'][i]\n",
    "        visualize_matrices([img.argmax(0)])\n",
    "        plot_matrices(masks, images_per_row=6)\n",
    "        print(\"attr: {},  label: {} c: {}\".format(attr, label, LABEL_TO_C_LABEL[label.item()]))\n",
    "        print()\n",
    "\n",
    "    for i in range(40):\n",
    "        img = d.data['test_unseen']['imgs'][i]\n",
    "        # masks = d.data['test_unseen']['masks'][i]\n",
    "        attr = d.data['test_unseen']['attributes'][i]\n",
    "        label = d.data['test_unseen']['labels'][i]\n",
    "        visualize_matrices([img.argmax(0)])\n",
    "        # plot_matrices(masks, images_per_row=6)\n",
    "        print(\"attr: {},  label: {} c: {}\".format(attr, label, LABEL_TO_C_LABEL[label.item()]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c46e2-aa2b-4796-ab24-1f8ad8a27ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
