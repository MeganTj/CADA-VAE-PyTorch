{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa50dcac-a674-43a5-817f-2e546df70b02",
   "metadata": {},
   "source": [
    "## Settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f730017e-838f-4747-aae1-34dd039f74bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 11:47 Loaded backend module://matplotlib_inline.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "### execute this function to train and test the vae-model\n",
    "import numpy as np\n",
    "import pdb\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "#vaemodel\n",
    "import copy\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.utils import data\n",
    "from data_loader import DATA_LOADER as dataloader\n",
    "import final_classifier as  classifier\n",
    "import models\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..', '..'))\n",
    "# BabyARC-fewshot dataset for classification:\n",
    "from reasoning.experiments.concept_energy import get_dataset, ConceptDataset\n",
    "from reasoning.pytorch_net.util import init_args, plot_matrices, get_device\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--dataset')\n",
    "parser.add_argument('--num_shots',type=int)\n",
    "parser.add_argument('--generalized', type = str2bool)\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('--batch_size', default=50, type=int)\n",
    "parser.add_argument('--gpuid', default=\"0\", type=str)\n",
    "\n",
    "try:\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    args = parser.parse_args([])\n",
    "    # Experiment management:\n",
    "    # args.dataset = \"AWA1\"\n",
    "    args.dataset = \"c-Line->Eshape\"\n",
    "    args.num_shots = 0\n",
    "    args.generalized = False\n",
    "    args.epochs = 5\n",
    "    args.gpuid = \"0\"\n",
    "    is_jupyter = True\n",
    "except:\n",
    "    args = parser.parse_args()\n",
    "    is_jupyter = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aece354-2f15-43fa-b400-1a3f0b58205f",
   "metadata": {},
   "source": [
    "## Model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "739dc545-5db3-43ed-9244-10ebb56a16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LINEAR_LOGSOFTMAX(nn.Module):\n",
    "    def __init__(self, input_dim, nclass):\n",
    "        super(LINEAR_LOGSOFTMAX, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim,nclass)\n",
    "        self.logic = nn.LogSoftmax(dim=1)\n",
    "        self.lossfunction =  nn.NLLLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.logic(self.fc(x))\n",
    "        return o\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self,hyperparameters):\n",
    "        super(Model,self).__init__()\n",
    "\n",
    "        self.device = hyperparameters['device']\n",
    "        self.auxiliary_data_source = hyperparameters['auxiliary_data_source']\n",
    "        self.all_data_sources  = ['resnet_features',self.auxiliary_data_source]\n",
    "        self.DATASET = hyperparameters['dataset']\n",
    "        self.num_shots = hyperparameters['num_shots']\n",
    "        self.latent_size = hyperparameters['latent_size']\n",
    "        self.batch_size = hyperparameters['batch_size']\n",
    "        self.hidden_size_rule = hyperparameters['hidden_size_rule']\n",
    "        self.warmup = hyperparameters['model_specifics']['warmup']\n",
    "        self.generalized = hyperparameters['generalized']\n",
    "        self.classifier_batch_size = 32\n",
    "        self.img_seen_samples   = hyperparameters['samples_per_class'][self.DATASET][0]\n",
    "        self.att_seen_samples   = hyperparameters['samples_per_class'][self.DATASET][1]\n",
    "        self.att_unseen_samples = hyperparameters['samples_per_class'][self.DATASET][2]\n",
    "        self.img_unseen_samples = hyperparameters['samples_per_class'][self.DATASET][3]\n",
    "        self.reco_loss_function = hyperparameters['loss']\n",
    "        self.nepoch = hyperparameters['epochs']\n",
    "        self.lr_cls = hyperparameters['lr_cls']\n",
    "        self.cross_reconstruction = hyperparameters['model_specifics']['cross_reconstruction']\n",
    "        self.cls_train_epochs = hyperparameters['cls_train_steps']\n",
    "        self.dataset = dataloader( self.DATASET, copy.deepcopy(self.auxiliary_data_source) , device= self.device )\n",
    "\n",
    "        feature_dimensions = [2048, self.dataset.aux_data.size(1)]\n",
    "        if self.DATASET=='CUB':\n",
    "            self.num_classes=200\n",
    "            self.num_novel_classes = 50\n",
    "        elif self.DATASET=='SUN':\n",
    "            self.num_classes=717\n",
    "            self.num_novel_classes = 72\n",
    "        elif self.DATASET=='AWA1' or self.DATASET=='AWA2':\n",
    "            self.num_classes=50\n",
    "            self.num_novel_classes = 10\n",
    "        elif self.DATASET.startswith(\"c-\"):\n",
    "            if self.DATASET=='c-Line->Eshape':\n",
    "                self.num_classes = 14\n",
    "                self.num_novel_classes = 3\n",
    "                feature_dimensions = [320, self.dataset.aux_data.size(1)]\n",
    "            else:\n",
    "                raise\n",
    "        else:\n",
    "            raise\n",
    "        \n",
    "        # Here, the encoders and decoders for all modalities are created and put into dict\n",
    "\n",
    "        self.encoder = {}\n",
    "\n",
    "        for datatype, dim in zip(self.all_data_sources, feature_dimensions):\n",
    "            if datatype == \"resnet_features\" and self.DATASET.startswith(\"c-\"):\n",
    "                self.encoder[datatype] = models.concept_encoder(dim, self.latent_size, self.hidden_size_rule[datatype], self.device)\n",
    "            else:\n",
    "                self.encoder[datatype] = models.encoder_template(dim, self.latent_size, self.hidden_size_rule[datatype], self.device)\n",
    "\n",
    "            print(str(datatype) + ' ' + str(dim))\n",
    "\n",
    "        self.decoder = {}\n",
    "        for datatype, dim in zip(self.all_data_sources, feature_dimensions):\n",
    "            if datatype == \"resnet_features\" and self.DATASET.startswith(\"c-\"):\n",
    "                self.decoder[datatype] = models.concept_decoder(self.latent_size, dim, self.hidden_size_rule[datatype], self.device)\n",
    "            else:\n",
    "                self.decoder[datatype] = models.decoder_template(self.latent_size, dim, self.hidden_size_rule[datatype], self.device)\n",
    "\n",
    "        # An optimizer for all encoders and decoders is defined here\n",
    "        parameters_to_optimize = list(self.parameters())\n",
    "        for datatype in self.all_data_sources:\n",
    "            parameters_to_optimize +=  list(self.encoder[datatype].parameters())\n",
    "            parameters_to_optimize +=  list(self.decoder[datatype].parameters())\n",
    "\n",
    "        self.optimizer  = optim.Adam( parameters_to_optimize ,lr=hyperparameters['lr_gen_model'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n",
    "\n",
    "        if self.reco_loss_function=='l2':\n",
    "            self.reconstruction_criterion = nn.MSELoss(size_average=False)\n",
    "\n",
    "        elif self.reco_loss_function=='l1':\n",
    "            self.reconstruction_criterion = nn.L1Loss(size_average=False)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.reparameterize_with_noise:\n",
    "            sigma = torch.exp(logvar)\n",
    "            eps = torch.FloatTensor(logvar.size()[0],1).normal_(0,1).to(device)\n",
    "            eps  = eps.expand(sigma.size())\n",
    "            return mu + sigma*eps\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def map_label(self,label, classes):\n",
    "        \"\"\"\n",
    "        label: [1,2,5,6]\n",
    "        \"\"\"\n",
    "        mapped_label = torch.LongTensor(label.size()).to(self.device)\n",
    "        for i in range(classes.size(0)):\n",
    "            mapped_label[label==classes[i]] = i\n",
    "\n",
    "        return mapped_label\n",
    "\n",
    "    def trainstep(self, img, att):\n",
    "\n",
    "        ##############################################\n",
    "        # Encode image features and additional\n",
    "        # features\n",
    "        ##############################################\n",
    "        mu_img, logvar_img = self.encoder['resnet_features'](img)  # [B, 64]\n",
    "        z_from_img = self.reparameterize(mu_img, logvar_img)  # [B, 64]\n",
    "\n",
    "        mu_att, logvar_att = self.encoder[self.auxiliary_data_source](att)\n",
    "        z_from_att = self.reparameterize(mu_att, logvar_att)\n",
    "\n",
    "        ##############################################\n",
    "        # Reconstruct inputs\n",
    "        ##############################################\n",
    "\n",
    "        img_from_img = self.decoder['resnet_features'](z_from_img)  # [B, 2048] \n",
    "        att_from_att = self.decoder[self.auxiliary_data_source](z_from_att)    # [B, 48]\n",
    "\n",
    "        reconstruction_loss = self.reconstruction_criterion(img_from_img, img) \\\n",
    "                              + self.reconstruction_criterion(att_from_att, att)\n",
    "\n",
    "        ##############################################\n",
    "        # Cross Reconstruction Loss\n",
    "        ##############################################\n",
    "        img_from_att = self.decoder['resnet_features'](z_from_att)\n",
    "        att_from_img = self.decoder[self.auxiliary_data_source](z_from_img)\n",
    "\n",
    "        cross_reconstruction_loss = self.reconstruction_criterion(img_from_att, img) \\\n",
    "                                    + self.reconstruction_criterion(att_from_img, att)\n",
    "\n",
    "        ##############################################\n",
    "        # KL-Divergence\n",
    "        ##############################################\n",
    "\n",
    "        KLD = (0.5 * torch.sum(1 + logvar_att - mu_att.pow(2) - logvar_att.exp())) \\\n",
    "              + (0.5 * torch.sum(1 + logvar_img - mu_img.pow(2) - logvar_img.exp()))\n",
    "\n",
    "        ##############################################\n",
    "        # Distribution Alignment\n",
    "        ##############################################\n",
    "        distance = torch.sqrt(torch.sum((mu_img - mu_att) ** 2, dim=1) + \\\n",
    "                              torch.sum((torch.sqrt(logvar_img.exp()) - torch.sqrt(logvar_att.exp())) ** 2, dim=1))\n",
    "\n",
    "        distance = distance.sum()\n",
    "\n",
    "        ##############################################\n",
    "        # scale the loss terms according to the warmup\n",
    "        # schedule\n",
    "        ##############################################\n",
    "\n",
    "        f1 = 1.0*(self.current_epoch - self.warmup['cross_reconstruction']['start_epoch'] )/(1.0*( self.warmup['cross_reconstruction']['end_epoch']- self.warmup['cross_reconstruction']['start_epoch']))\n",
    "        f1 = f1*(1.0*self.warmup['cross_reconstruction']['factor'])\n",
    "        cross_reconstruction_factor = torch.tensor([min(max(f1,0),self.warmup['cross_reconstruction']['factor'])], dtype=torch.float32, device=device)\n",
    "\n",
    "        f2 = 1.0 * (self.current_epoch - self.warmup['beta']['start_epoch']) / ( 1.0 * (self.warmup['beta']['end_epoch'] - self.warmup['beta']['start_epoch']))\n",
    "        f2 = f2 * (1.0 * self.warmup['beta']['factor'])\n",
    "        beta = torch.tensor([min(max(f2, 0), self.warmup['beta']['factor'])], dtype=torch.float32, device=device)\n",
    "\n",
    "        f3 = 1.0*(self.current_epoch - self.warmup['distance']['start_epoch'] )/(1.0*( self.warmup['distance']['end_epoch']- self.warmup['distance']['start_epoch']))\n",
    "        f3 = f3*(1.0*self.warmup['distance']['factor'])\n",
    "        distance_factor = torch.tensor([min(max(f3,0),self.warmup['distance']['factor'])], dtype=torch.float32, device=device)\n",
    "\n",
    "        ##############################################\n",
    "        # Put the loss together and call the optimizer\n",
    "        ##############################################\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss = reconstruction_loss - beta * KLD\n",
    "\n",
    "        if cross_reconstruction_loss>0:\n",
    "            loss += cross_reconstruction_factor*cross_reconstruction_loss\n",
    "        if distance_factor >0:\n",
    "            loss += distance_factor*distance\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train_vae(self):\n",
    "\n",
    "        losses = []\n",
    "        # self.dataloader = data.DataLoader(self.dataset,batch_size= self.batch_size,shuffle= True,drop_last=True)#,num_workers = 4)\n",
    "\n",
    "        self.dataset.novelclasses =self.dataset.novelclasses.long().to(device)\n",
    "        self.dataset.seenclasses =self.dataset.seenclasses.long().to(device)\n",
    "        #leave both statements\n",
    "        self.train()\n",
    "        self.reparameterize_with_noise = True\n",
    "\n",
    "        print('train for reconstruction')\n",
    "        for epoch in range(0, self.nepoch ):\n",
    "            self.current_epoch = epoch\n",
    "\n",
    "            i=-1\n",
    "            for iters in range(0, self.dataset.ntrain, self.batch_size):\n",
    "                i+=1\n",
    "\n",
    "                label, data_from_modalities = self.dataset.next_batch(self.batch_size)  # label: [B], data_from_modalities: ([B, F_img:2048], [B, F_attr:85])\n",
    "\n",
    "                label= label.long().to(self.device)\n",
    "                for j in range(len(data_from_modalities)):\n",
    "                    data_from_modalities[j] = data_from_modalities[j].to(self.device)\n",
    "                    data_from_modalities[j].requires_grad = False\n",
    "\n",
    "                loss = self.trainstep(data_from_modalities[0], data_from_modalities[1] )\n",
    "\n",
    "                if i%50==0:\n",
    "\n",
    "                    print('epoch ' + str(epoch) + ' | iter ' + str(i) + '\\t'+\n",
    "                    ' | loss ' +  str(loss)[:5]   )\n",
    "\n",
    "                if i%50==0 and i>0:\n",
    "                    losses.append(loss)\n",
    "\n",
    "        # turn into evaluation mode:\n",
    "        for key, value in self.encoder.items():\n",
    "            self.encoder[key].eval()\n",
    "        for key, value in self.decoder.items():\n",
    "            self.decoder[key].eval()\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def train_classifier(self, show_plots=False):\n",
    "\n",
    "        if self.num_shots > 0 :\n",
    "            print('================  transfer features from test to train ==================')\n",
    "            self.dataset.transfer_features(self.num_shots, num_queries='num_features')\n",
    "\n",
    "        history = []  # stores accuracies\n",
    "\n",
    "\n",
    "        cls_seenclasses = self.dataset.seenclasses\n",
    "        cls_novelclasses = self.dataset.novelclasses\n",
    "\n",
    "        train_seen_feat = self.dataset.data['train_seen']['resnet_features']  # [19832, 2048]\n",
    "        train_seen_label = self.dataset.data['train_seen']['labels']  # [19832]\n",
    "\n",
    "        novelclass_aux_data = self.dataset.novelclass_aux_data  # [10, F_attr:85], access as novelclass_aux_data['resnet_features'], novelclass_aux_data['attributes']\n",
    "        seenclass_aux_data = self.dataset.seenclass_aux_data  # [40, F_attr:85]\n",
    "\n",
    "        novel_corresponding_labels = self.dataset.novelclasses.long().to(self.device)  # [10]\n",
    "        seen_corresponding_labels = self.dataset.seenclasses.long().to(self.device)  #[40]\n",
    "\n",
    "\n",
    "        # The resnet_features for testing the classifier are loaded here\n",
    "        novel_test_feat = self.dataset.data['test_unseen'][\n",
    "            'resnet_features']  # [5685, 2048], self.dataset.test_novel_feature.to(self.device)\n",
    "        seen_test_feat = self.dataset.data['test_seen'][\n",
    "            'resnet_features']  # [4958, 2048] self.dataset.test_seen_feature.to(self.device)\n",
    "        test_seen_label = self.dataset.data['test_seen']['labels']  # [4598], self.dataset.test_seen_label.to(self.device)\n",
    "        test_novel_label = self.dataset.data['test_unseen']['labels']  # [5685], self.dataset.test_novel_label.to(self.device)\n",
    "\n",
    "        train_unseen_feat = self.dataset.data['train_unseen']['resnet_features']  # None\n",
    "        train_unseen_label = self.dataset.data['train_unseen']['labels']  # None\n",
    "\n",
    "\n",
    "        # in ZSL mode:\n",
    "        if self.generalized == False:\n",
    "            # there are only 50 classes in ZSL (for CUB)\n",
    "            # novel_corresponding_labels =list of all novel classes (as tensor)\n",
    "            # test_novel_label = mapped to 0-49 in classifier function\n",
    "            # those are used as targets, they have to be mapped to 0-49 right here:\n",
    "            novel_corresponding_labels = self.map_label(novel_corresponding_labels, novel_corresponding_labels)  # before: [ 6,  8, 22, 23, 29, 30, 33, 40, 46, 49]; after: [0,1,...9]\n",
    "\n",
    "            if self.num_shots > 0:\n",
    "                # not generalized and at least 1 shot means normal FSL setting (use only unseen classes)\n",
    "                train_unseen_label = self.map_label(train_unseen_label, cls_novelclasses)  # train_unseen_label: shape [5685]: [29, 29, 29,  ..., 46, 46, 46]\n",
    "\n",
    "            # for FSL, we train_seen contains the unseen class examples\n",
    "            # for ZSL, train seen label is not used\n",
    "            # if self.num_shots>0:\n",
    "            #    train_seen_label = self.map_label(train_seen_label,cls_novelclasses)\n",
    "\n",
    "            test_novel_label = self.map_label(test_novel_label, cls_novelclasses)  # [5685]\n",
    "\n",
    "            # map cls novelclasses last\n",
    "            cls_novelclasses = self.map_label(cls_novelclasses, cls_novelclasses)  # [10], [0,1,...9]\n",
    "\n",
    "\n",
    "        if self.generalized:\n",
    "            print('mode: gzsl')\n",
    "            clf = LINEAR_LOGSOFTMAX(self.latent_size, self.num_classes)\n",
    "        else:\n",
    "            print('mode: zsl')\n",
    "            clf = LINEAR_LOGSOFTMAX(self.latent_size, self.num_novel_classes)\n",
    "\n",
    "\n",
    "        clf.apply(models.weights_init)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ####################################\n",
    "            # preparing the test set\n",
    "            # convert raw test data into z vectors\n",
    "            ####################################\n",
    "\n",
    "            self.reparameterize_with_noise = False\n",
    "\n",
    "            mu1, var1 = self.encoder['resnet_features'](novel_test_feat)  # novel_test_feat: [5685, 2048]\n",
    "            test_novel_X = self.reparameterize(mu1, var1).to(self.device).data\n",
    "            test_novel_Y = test_novel_label.to(self.device)\n",
    "\n",
    "            if len(seen_test_feat) > 0:\n",
    "                mu2, var2 = self.encoder['resnet_features'](seen_test_feat)  # mu2: [4958, 64]\n",
    "                test_seen_X = self.reparameterize(mu2, var2).to(self.device).data\n",
    "                test_seen_Y = test_seen_label.to(self.device)\n",
    "            else:\n",
    "                test_seen_X = torch.tensor([]).to(self.device)\n",
    "                test_seen_Y = torch.tensor([]).to(self.device)\n",
    "                \n",
    "\n",
    "            ####################################\n",
    "            # preparing the train set:\n",
    "            # chose n random image features per\n",
    "            # class. If n exceeds the number of\n",
    "            # image features per class, duplicate\n",
    "            # some. Next, convert them to\n",
    "            # latent z features.\n",
    "            ####################################\n",
    "\n",
    "            self.reparameterize_with_noise = True\n",
    "\n",
    "            def sample_train_data_on_sample_per_class_basis(features, label, sample_per_class):\n",
    "                sample_per_class = int(sample_per_class)\n",
    "\n",
    "                if sample_per_class != 0 and len(label) != 0:\n",
    "\n",
    "                    classes = label.unique()\n",
    "\n",
    "                    for i, s in enumerate(classes):\n",
    "\n",
    "                        features_of_that_class = features[label == s, :]  # order of features and labels must coincide\n",
    "                        # if number of selected features is smaller than the number of features we want per class:\n",
    "                        multiplier = torch.ceil(torch.tensor(\n",
    "                            [max(1, sample_per_class / features_of_that_class.size(0))], dtype=torch.float32, device=device)).long().item()\n",
    "\n",
    "                        features_of_that_class = features_of_that_class.repeat(multiplier, 1)\n",
    "\n",
    "                        if i == 0:\n",
    "                            features_to_return = features_of_that_class[:sample_per_class, :]\n",
    "                            labels_to_return = s.repeat(sample_per_class)\n",
    "                        else:\n",
    "                            features_to_return = torch.cat(\n",
    "                                (features_to_return, features_of_that_class[:sample_per_class, :]), dim=0)\n",
    "                            labels_to_return = torch.cat((labels_to_return, s.repeat(sample_per_class)),\n",
    "                                                         dim=0)\n",
    "\n",
    "                    return features_to_return, labels_to_return\n",
    "                else:\n",
    "                    return torch.tensor([], device=device), torch.tensor([], dtype=torch.int64, device=device)\n",
    "\n",
    "\n",
    "            # some of the following might be empty tensors if the specified number of\n",
    "            # samples is zero :\n",
    "\n",
    "            img_seen_feat,   img_seen_label   = sample_train_data_on_sample_per_class_basis(\n",
    "                train_seen_feat,train_seen_label,self.img_seen_samples )  # tensor([]), tensor([])\n",
    "\n",
    "            img_unseen_feat, img_unseen_label = sample_train_data_on_sample_per_class_basis(\n",
    "                train_unseen_feat, train_unseen_label, self.img_unseen_samples )  # tensor([]), tensor([])\n",
    "\n",
    "            att_unseen_feat, att_unseen_label = sample_train_data_on_sample_per_class_basis(\n",
    "                    novelclass_aux_data,\n",
    "                    novel_corresponding_labels,self.att_unseen_samples )  # [2000, 85], [2000]\n",
    "\n",
    "            att_seen_feat, att_seen_label = sample_train_data_on_sample_per_class_basis(\n",
    "                seenclass_aux_data,\n",
    "                seen_corresponding_labels, self.att_seen_samples)  # tensor([]), tensor([])\n",
    "\n",
    "            def convert_datapoints_to_z(features, encoder):\n",
    "                if features.size(0) != 0:\n",
    "                    mu_, logvar_ = encoder(features)\n",
    "                    z = self.reparameterize(mu_, logvar_)\n",
    "                    return z\n",
    "                else:\n",
    "                    return torch.tensor([], dtype=torch.float32, device=device)\n",
    "\n",
    "            z_seen_img   = convert_datapoints_to_z(img_seen_feat, self.encoder['resnet_features'])\n",
    "            z_unseen_img = convert_datapoints_to_z(img_unseen_feat, self.encoder['resnet_features'])\n",
    "\n",
    "            z_seen_att = convert_datapoints_to_z(att_seen_feat, self.encoder[self.auxiliary_data_source])\n",
    "            z_unseen_att = convert_datapoints_to_z(att_unseen_feat, self.encoder[self.auxiliary_data_source])  # [2000, 64]\n",
    "\n",
    "            train_Z = [z_seen_img, z_unseen_img ,z_seen_att    ,z_unseen_att] # only z_seen_att is not None, has shape [2000, 85]\n",
    "            train_L = [img_seen_label    , img_unseen_label,att_seen_label,att_unseen_label]\n",
    "\n",
    "            # empty tensors are sorted out\n",
    "            train_X = [train_Z[i] for i in range(len(train_Z)) if train_Z[i].size(0) != 0]\n",
    "            train_Y = [train_L[i] for i in range(len(train_L)) if train_Z[i].size(0) != 0]\n",
    "\n",
    "            train_X = torch.cat(train_X, dim=0)  # [2000, 64]\n",
    "            train_Y = torch.cat(train_Y, dim=0)\n",
    "\n",
    "        ############################################################\n",
    "        ##### initializing the classifier and train one epoch\n",
    "        ############################################################\n",
    "\n",
    "        # test_seen_X: [4958, 64]\n",
    "        # test_seen_Y:  [4958]\n",
    "        # test_novel_X: [5685, 64]\n",
    "        # test_novel_Y: [5685]\n",
    "        # cls_seenclasses: [40]\n",
    "        # cls_novelclasses: [10]: [0,1,...9]\n",
    "        cls = classifier.CLASSIFIER(clf, train_X, train_Y, test_seen_X, test_seen_Y, test_novel_X,\n",
    "                                    test_novel_Y,\n",
    "                                    cls_seenclasses, cls_novelclasses,\n",
    "                                    self.num_classes, self.device, self.lr_cls, 0.5, 1,\n",
    "                                    self.classifier_batch_size,\n",
    "                                    self.generalized)\n",
    "\n",
    "        for k in range(self.cls_train_epochs):\n",
    "            if k > 0:\n",
    "                if self.generalized:\n",
    "                    cls.acc_seen, cls.acc_novel, cls.H = cls.fit()\n",
    "                else:\n",
    "                    cls.acc = cls.fit_zsl()\n",
    "\n",
    "            if self.generalized:\n",
    "\n",
    "                print('[%.1f]     novel=%.4f, seen=%.4f, h=%.4f , loss=%.4f' % (\n",
    "                k, cls.acc_novel, cls.acc_seen, cls.H, cls.average_loss))\n",
    "\n",
    "                history.append([torch.tensor(cls.acc_seen).item(), torch.tensor(cls.acc_novel).item(),\n",
    "                                torch.tensor(cls.H).item()])\n",
    "\n",
    "            else:\n",
    "                print('[%.1f]  acc=%.4f ' % (k, cls.acc))\n",
    "                history.append([0, torch.tensor(cls.acc).item(), 0])\n",
    "\n",
    "        if self.generalized:\n",
    "            return torch.tensor(cls.acc_seen).item(), torch.tensor(cls.acc_novel).item(), torch.tensor(\n",
    "                cls.H).item(), history\n",
    "        else:\n",
    "            return 0, torch.tensor(cls.acc).item(), 0, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc2131-f243-4f5c-8cc0-6e49205ca8bc",
   "metadata": {},
   "source": [
    "## Model init:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7fa3617-5217-449f-9d97-bf2f71932118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_shots': 0, 'device': device(type='cuda', index=0), 'model_specifics': {'cross_reconstruction': True, 'name': 'CADA', 'distance': 'wasserstein', 'warmup': {'beta': {'factor': 0.25, 'end_epoch': 93, 'start_epoch': 0}, 'cross_reconstruction': {'factor': 2.37, 'end_epoch': 75, 'start_epoch': 21}, 'distance': {'factor': 8.13, 'end_epoch': 22, 'start_epoch': 6}}}, 'lr_gen_model': 0.00015, 'generalized': False, 'batch_size': 50, 'xyu_samples_per_class': {'SUN': (200, 0, 400, 0), 'APY': (200, 0, 400, 0), 'CUB': (200, 0, 400, 0), 'AWA2': (200, 0, 400, 0), 'FLO': (200, 0, 400, 0), 'AWA1': (200, 0, 400, 0)}, 'epochs': 5, 'loss': 'l1', 'auxiliary_data_source': 'attributes', 'lr_cls': 0.001, 'dataset': 'c-Line->Eshape', 'hidden_size_rule': {'resnet_features': (1560, 1660), 'attributes': (1450, 665), 'sentences': (1450, 665)}, 'latent_size': 64}\n",
      "{'dataset': 'c-Line->Eshape', 'num_shots': 0, 'generalized': False, 'cls_train_steps': 21}\n",
      "***\n",
      "21\n",
      "The current working directory is\n",
      "/lfs/hyperturing1/0/tailin/.src/reasoning/CADA_VAE_PyTorch/model\n",
      "Project Directory:\n",
      "/lfs/hyperturing1/0/tailin/.src/reasoning/CADA_VAE_PyTorch\n",
      "Data Path\n",
      "/lfs/hyperturing1/0/tailin/.src/reasoning/CADA_VAE_PyTorch/data\n",
      "[2022-01-24 11:47:43] Dataset loaded from /lfs/hyperturing1/0/tailin/.results/Datasets/c-Line-ex_44000_seed_1_cav_16_rain_0.0_color_1,2.p.\n",
      "[2022-01-24 11:47:55] Dataset loaded from /lfs/hyperturing1/0/tailin/.results/Datasets/c-Parallel+VerticalMid+VerticalEdge-ex_44000_seed_1_cav_16_rain_0.0_color_1,2_distr_3.p.\n",
      "[2022-01-24 11:47:55] Dataset loaded from /lfs/hyperturing1/0/tailin/.results/Datasets/c-Eshape+Fshape+Ashape-ex_400_seed_2_cav_16_rain_0.0_color_1,2_distr_0.p.\n",
      "resnet_features 320\n",
      "attributes 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lfs/hyperturing1/0/tailin/miniconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (reconstruction_criterion): L1Loss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################\n",
    "# the basic hyperparameters\n",
    "########################################\n",
    "device = get_device(init_args({\"gpuid\": args.gpuid}))\n",
    "hyperparameters = {\n",
    "    'num_shots': 0,\n",
    "    'device': device,\n",
    "    'model_specifics': {'cross_reconstruction': True,\n",
    "                       'name': 'CADA',\n",
    "                       'distance': 'wasserstein',\n",
    "                       'warmup': {'beta': {'factor': 0.25,\n",
    "                                           'end_epoch': 93,\n",
    "                                           'start_epoch': 0},\n",
    "                                  'cross_reconstruction': {'factor': 2.37,\n",
    "                                                           'end_epoch': 75,\n",
    "                                                           'start_epoch': 21},\n",
    "                                  'distance': {'factor': 8.13,\n",
    "                                               'end_epoch': 22,\n",
    "                                               'start_epoch': 6}}},\n",
    "\n",
    "    'lr_gen_model': 0.00015,\n",
    "    'generalized': True,\n",
    "    'batch_size': args.batch_size,\n",
    "    'xyu_samples_per_class': {'SUN': (200, 0, 400, 0),\n",
    "                              'APY': (200, 0, 400, 0),\n",
    "                              'CUB': (200, 0, 400, 0),\n",
    "                              'AWA2': (200, 0, 400, 0),\n",
    "                              'FLO': (200, 0, 400, 0),\n",
    "                              'AWA1': (200, 0, 400, 0)},\n",
    "    'epochs': args.epochs,\n",
    "    'loss': 'l1',\n",
    "    'auxiliary_data_source' : 'attributes',\n",
    "    'lr_cls': 0.001,\n",
    "    'dataset': 'CUB',\n",
    "    'hidden_size_rule': {'resnet_features': (1560, 1660),\n",
    "                        'attributes': (1450, 665),\n",
    "                        'sentences': (1450, 665) },\n",
    "    'latent_size': 64\n",
    "}\n",
    "\n",
    "# The training epochs for the final classifier, for early stopping,\n",
    "# as determined on the validation spit\n",
    "\n",
    "cls_train_steps = [\n",
    "      {'dataset': 'c-Line->Eshape',  'num_shots': 0, 'generalized': False, 'cls_train_steps': 21},\n",
    "      {'dataset': 'SUN',  'num_shots': 0, 'generalized': True, 'cls_train_steps': 21},\n",
    "      {'dataset': 'SUN',  'num_shots': 0, 'generalized': False, 'cls_train_steps': 30},\n",
    "      {'dataset': 'SUN',  'num_shots': 1, 'generalized': True, 'cls_train_steps': 22},\n",
    "      {'dataset': 'SUN',  'num_shots': 1, 'generalized': False, 'cls_train_steps': 96},\n",
    "      {'dataset': 'SUN',  'num_shots': 5, 'generalized': True, 'cls_train_steps': 29},\n",
    "      {'dataset': 'SUN',  'num_shots': 5, 'generalized': False, 'cls_train_steps': 78},\n",
    "      {'dataset': 'SUN',  'num_shots': 2, 'generalized': True, 'cls_train_steps': 29},\n",
    "      {'dataset': 'SUN',  'num_shots': 2, 'generalized': False, 'cls_train_steps': 61},\n",
    "      {'dataset': 'SUN',  'num_shots': 10, 'generalized': True, 'cls_train_steps': 79},\n",
    "      {'dataset': 'SUN',  'num_shots': 10, 'generalized': False, 'cls_train_steps': 94},\n",
    "      {'dataset': 'AWA1', 'num_shots': 0, 'generalized': True, 'cls_train_steps': 33},\n",
    "      {'dataset': 'AWA1', 'num_shots': 0, 'generalized': False, 'cls_train_steps': 25},\n",
    "      {'dataset': 'AWA1', 'num_shots': 1, 'generalized': True, 'cls_train_steps': 40},\n",
    "      {'dataset': 'AWA1', 'num_shots': 1, 'generalized': False, 'cls_train_steps': 81},\n",
    "      {'dataset': 'AWA1', 'num_shots': 5, 'generalized': True, 'cls_train_steps': 89},\n",
    "      {'dataset': 'AWA1', 'num_shots': 5, 'generalized': False, 'cls_train_steps': 62},\n",
    "      {'dataset': 'AWA1', 'num_shots': 2, 'generalized': True, 'cls_train_steps': 56},\n",
    "      {'dataset': 'AWA1', 'num_shots': 2, 'generalized': False, 'cls_train_steps': 59},\n",
    "      {'dataset': 'AWA1', 'num_shots': 10, 'generalized': True, 'cls_train_steps': 100},\n",
    "      {'dataset': 'AWA1', 'num_shots': 10, 'generalized': False, 'cls_train_steps': 50},\n",
    "      {'dataset': 'CUB',  'num_shots': 0, 'generalized': True, 'cls_train_steps': 23},\n",
    "      {'dataset': 'CUB',  'num_shots': 0, 'generalized': False, 'cls_train_steps': 22},\n",
    "      {'dataset': 'CUB',  'num_shots': 1, 'generalized': True, 'cls_train_steps': 34},\n",
    "      {'dataset': 'CUB',  'num_shots': 1, 'generalized': False, 'cls_train_steps': 46},\n",
    "      {'dataset': 'CUB',  'num_shots': 5, 'generalized': True, 'cls_train_steps': 64},\n",
    "      {'dataset': 'CUB',  'num_shots': 5, 'generalized': False, 'cls_train_steps': 73},\n",
    "      {'dataset': 'CUB',  'num_shots': 2, 'generalized': True, 'cls_train_steps': 39},\n",
    "      {'dataset': 'CUB',  'num_shots': 2, 'generalized': False, 'cls_train_steps': 31},\n",
    "      {'dataset': 'CUB',  'num_shots': 10, 'generalized': True, 'cls_train_steps': 85},\n",
    "      {'dataset': 'CUB',  'num_shots': 10, 'generalized': False, 'cls_train_steps': 67},\n",
    "      {'dataset': 'AWA2', 'num_shots': 0, 'generalized': True, 'cls_train_steps': 29},\n",
    "      {'dataset': 'AWA2', 'num_shots': 0, 'generalized': False, 'cls_train_steps': 39},\n",
    "      {'dataset': 'AWA2', 'num_shots': 1, 'generalized': True, 'cls_train_steps': 44},\n",
    "      {'dataset': 'AWA2', 'num_shots': 1, 'generalized': False, 'cls_train_steps': 96},\n",
    "      {'dataset': 'AWA2', 'num_shots': 5, 'generalized': True, 'cls_train_steps': 99},\n",
    "      {'dataset': 'AWA2', 'num_shots': 5, 'generalized': False, 'cls_train_steps': 100},\n",
    "      {'dataset': 'AWA2', 'num_shots': 2, 'generalized': True, 'cls_train_steps': 69},\n",
    "      {'dataset': 'AWA2', 'num_shots': 2, 'generalized': False, 'cls_train_steps': 79},\n",
    "      {'dataset': 'AWA2', 'num_shots': 10, 'generalized': True, 'cls_train_steps': 86},\n",
    "      {'dataset': 'AWA2', 'num_shots': 10, 'generalized': False, 'cls_train_steps': 78}\n",
    "]\n",
    "\n",
    "##################################\n",
    "# change some hyperparameters here\n",
    "##################################\n",
    "hyperparameters['dataset'] = args.dataset\n",
    "hyperparameters['num_shots']= args.num_shots\n",
    "hyperparameters['generalized']= args.generalized\n",
    "print(hyperparameters)\n",
    "print(cls_train_steps[0])\n",
    "hyperparameters['cls_train_steps'] = [x['cls_train_steps']  for x in cls_train_steps\n",
    "                                        if all([hyperparameters['dataset']==x['dataset'],\n",
    "                                        hyperparameters['num_shots']==x['num_shots'],\n",
    "                                        hyperparameters['generalized']==x['generalized'] ])][0]\n",
    "\n",
    "print('***')\n",
    "print(hyperparameters['cls_train_steps'] )\n",
    "if hyperparameters['generalized']:\n",
    "    if hyperparameters['num_shots']==0:\n",
    "        hyperparameters['samples_per_class'] = {'CUB': (200, 0, 400, 0), 'SUN': (200, 0, 400, 0),\n",
    "                                'APY': (200, 0,  400, 0), 'AWA1': (200, 0, 400, 0),\n",
    "                                'AWA2': (200, 0, 400, 0), 'FLO': (200, 0, 400, 0)}\n",
    "    else:\n",
    "        hyperparameters['samples_per_class'] = {'CUB': (200, 0, 200, 200), 'SUN': (200, 0, 200, 200),\n",
    "                                                    'APY': (200, 0, 200, 200), 'AWA1': (200, 0, 200, 200),\n",
    "                                                    'AWA2': (200, 0, 200, 200), 'FLO': (200, 0, 200, 200)}\n",
    "else:\n",
    "    if hyperparameters['num_shots']==0:\n",
    "        hyperparameters['samples_per_class'] = {'CUB': (0, 0, 200, 0), 'SUN': (0, 0, 200, 0),\n",
    "                                                    'APY': (0, 0, 200, 0), 'AWA1': (0, 0, 200, 0),\n",
    "                                                    'AWA2': (0, 0, 200, 0), 'FLO': (0, 0, 200, 0),\n",
    "                                                    'c-Line->Eshape': (0, 0, 200, 0),\n",
    "                                               }\n",
    "    else:\n",
    "        hyperparameters['samples_per_class'] = {'CUB': (0, 0, 200, 200), 'SUN': (0, 0, 200, 200),\n",
    "                                                    'APY': (0, 0, 200, 200), 'AWA1': (0, 0, 200, 200),\n",
    "                                                    'AWA2': (0, 0, 200, 200), 'FLO': (0, 0, 200, 200)}\n",
    "\n",
    "\n",
    "model = Model( hyperparameters)\n",
    "model.to(hyperparameters['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ead76f-1559-46b6-ab28-9e567221fe13",
   "metadata": {},
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad07c095-157d-46e6-a5aa-03ea462b6533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train for reconstruction\n",
      "epoch 0 | iter 0\t | loss 4917.\n",
      "epoch 0 | iter 50\t | loss 2759.\n",
      "epoch 0 | iter 100\t | loss 2557.\n",
      "epoch 0 | iter 150\t | loss 2076.\n",
      "epoch 0 | iter 200\t | loss 1799.\n",
      "epoch 0 | iter 250\t | loss 1760.\n",
      "epoch 0 | iter 300\t | loss 1613.\n",
      "epoch 0 | iter 350\t | loss 1573.\n",
      "epoch 0 | iter 400\t | loss 1436.\n",
      "epoch 0 | iter 450\t | loss 1296.\n",
      "epoch 0 | iter 500\t | loss 1236.\n",
      "epoch 0 | iter 550\t | loss 1147.\n",
      "epoch 0 | iter 600\t | loss 1115.\n",
      "epoch 0 | iter 650\t | loss 1085.\n",
      "epoch 0 | iter 700\t | loss 1040.\n",
      "epoch 0 | iter 750\t | loss 1019.\n",
      "epoch 0 | iter 800\t | loss 979.5\n",
      "epoch 0 | iter 850\t | loss 1011.\n",
      "epoch 0 | iter 900\t | loss 1001.\n",
      "epoch 0 | iter 950\t | loss 985.1\n",
      "epoch 0 | iter 1000\t | loss 959.5\n",
      "epoch 0 | iter 1050\t | loss 946.3\n",
      "epoch 0 | iter 1100\t | loss 956.3\n",
      "epoch 0 | iter 1150\t | loss 903.9\n",
      "epoch 0 | iter 1200\t | loss 935.1\n",
      "epoch 0 | iter 1250\t | loss 922.5\n",
      "epoch 0 | iter 1300\t | loss 912.3\n",
      "epoch 0 | iter 1350\t | loss 882.8\n",
      "epoch 0 | iter 1400\t | loss 885.8\n",
      "epoch 0 | iter 1450\t | loss 869.0\n",
      "epoch 0 | iter 1500\t | loss 877.9\n",
      "epoch 0 | iter 1550\t | loss 862.5\n",
      "epoch 0 | iter 1600\t | loss 825.0\n",
      "epoch 0 | iter 1650\t | loss 842.6\n",
      "epoch 0 | iter 1700\t | loss 846.9\n",
      "epoch 0 | iter 1750\t | loss 847.0\n",
      "epoch 1 | iter 0\t | loss 845.3\n",
      "epoch 1 | iter 50\t | loss 844.0\n",
      "epoch 1 | iter 100\t | loss 848.4\n",
      "epoch 1 | iter 150\t | loss 822.7\n",
      "epoch 1 | iter 200\t | loss 853.1\n",
      "epoch 1 | iter 250\t | loss 791.5\n",
      "epoch 1 | iter 300\t | loss 789.0\n",
      "epoch 1 | iter 350\t | loss 776.7\n",
      "epoch 1 | iter 400\t | loss 810.2\n",
      "epoch 1 | iter 450\t | loss 803.7\n",
      "epoch 1 | iter 500\t | loss 793.7\n",
      "epoch 1 | iter 550\t | loss 793.9\n",
      "epoch 1 | iter 600\t | loss 756.6\n",
      "epoch 1 | iter 650\t | loss 778.3\n",
      "epoch 1 | iter 700\t | loss 778.3\n",
      "epoch 1 | iter 750\t | loss 751.9\n",
      "epoch 1 | iter 800\t | loss 738.5\n",
      "epoch 1 | iter 850\t | loss 729.9\n",
      "epoch 1 | iter 900\t | loss 761.6\n",
      "epoch 1 | iter 950\t | loss 752.6\n",
      "epoch 1 | iter 1000\t | loss 741.6\n",
      "epoch 1 | iter 1050\t | loss 732.9\n",
      "epoch 1 | iter 1100\t | loss 726.8\n",
      "epoch 1 | iter 1150\t | loss 724.3\n",
      "epoch 1 | iter 1200\t | loss 752.2\n",
      "epoch 1 | iter 1250\t | loss 716.3\n",
      "epoch 1 | iter 1300\t | loss 720.2\n",
      "epoch 1 | iter 1350\t | loss 700.7\n",
      "epoch 1 | iter 1400\t | loss 732.5\n",
      "epoch 1 | iter 1450\t | loss 709.8\n",
      "epoch 1 | iter 1500\t | loss 720.4\n",
      "epoch 1 | iter 1550\t | loss 725.9\n",
      "epoch 1 | iter 1600\t | loss 699.8\n",
      "epoch 1 | iter 1650\t | loss 713.2\n",
      "epoch 1 | iter 1700\t | loss 695.3\n",
      "epoch 1 | iter 1750\t | loss 713.1\n",
      "epoch 2 | iter 0\t | loss 718.5\n",
      "epoch 2 | iter 50\t | loss 724.8\n",
      "epoch 2 | iter 100\t | loss 715.5\n",
      "epoch 2 | iter 150\t | loss 696.0\n",
      "epoch 2 | iter 200\t | loss 671.9\n",
      "epoch 2 | iter 250\t | loss 695.9\n",
      "epoch 2 | iter 300\t | loss 691.8\n",
      "epoch 2 | iter 350\t | loss 690.7\n",
      "epoch 2 | iter 400\t | loss 684.6\n",
      "epoch 2 | iter 450\t | loss 673.7\n",
      "epoch 2 | iter 500\t | loss 698.8\n",
      "epoch 2 | iter 550\t | loss 702.9\n",
      "epoch 2 | iter 600\t | loss 693.9\n",
      "epoch 2 | iter 650\t | loss 665.5\n",
      "epoch 2 | iter 700\t | loss 714.5\n",
      "epoch 2 | iter 750\t | loss 654.3\n",
      "epoch 2 | iter 800\t | loss 651.7\n",
      "epoch 2 | iter 850\t | loss 669.5\n",
      "epoch 2 | iter 900\t | loss 670.8\n",
      "epoch 2 | iter 950\t | loss 666.9\n",
      "epoch 2 | iter 1000\t | loss 663.4\n",
      "epoch 2 | iter 1050\t | loss 678.4\n",
      "epoch 2 | iter 1100\t | loss 682.7\n",
      "epoch 2 | iter 1150\t | loss 670.8\n",
      "epoch 2 | iter 1200\t | loss 668.4\n",
      "epoch 2 | iter 1250\t | loss 653.1\n",
      "epoch 2 | iter 1300\t | loss 677.6\n",
      "epoch 2 | iter 1350\t | loss 655.4\n",
      "epoch 2 | iter 1400\t | loss 661.0\n",
      "epoch 2 | iter 1450\t | loss 621.5\n",
      "epoch 2 | iter 1500\t | loss 626.0\n",
      "epoch 2 | iter 1550\t | loss 646.6\n",
      "epoch 2 | iter 1600\t | loss 644.2\n",
      "epoch 2 | iter 1650\t | loss 664.3\n",
      "epoch 2 | iter 1700\t | loss 692.9\n",
      "epoch 2 | iter 1750\t | loss 640.1\n",
      "epoch 3 | iter 0\t | loss 628.0\n",
      "epoch 3 | iter 50\t | loss 657.1\n",
      "epoch 3 | iter 100\t | loss 648.2\n",
      "epoch 3 | iter 150\t | loss 632.4\n",
      "epoch 3 | iter 200\t | loss 632.8\n",
      "epoch 3 | iter 250\t | loss 632.7\n",
      "epoch 3 | iter 300\t | loss 655.9\n",
      "epoch 3 | iter 350\t | loss 652.9\n",
      "epoch 3 | iter 400\t | loss 658.1\n",
      "epoch 3 | iter 450\t | loss 678.7\n",
      "epoch 3 | iter 500\t | loss 649.4\n",
      "epoch 3 | iter 550\t | loss 656.8\n",
      "epoch 3 | iter 600\t | loss 628.3\n",
      "epoch 3 | iter 650\t | loss 630.2\n",
      "epoch 3 | iter 700\t | loss 648.2\n",
      "epoch 3 | iter 750\t | loss 653.1\n",
      "epoch 3 | iter 800\t | loss 664.8\n",
      "epoch 3 | iter 850\t | loss 640.1\n",
      "epoch 3 | iter 900\t | loss 652.1\n",
      "epoch 3 | iter 950\t | loss 609.1\n",
      "epoch 3 | iter 1000\t | loss 641.2\n",
      "epoch 3 | iter 1050\t | loss 646.4\n",
      "epoch 3 | iter 1100\t | loss 635.1\n",
      "epoch 3 | iter 1150\t | loss 624.5\n",
      "epoch 3 | iter 1200\t | loss 652.9\n",
      "epoch 3 | iter 1250\t | loss 619.8\n",
      "epoch 3 | iter 1300\t | loss 628.5\n",
      "epoch 3 | iter 1350\t | loss 618.4\n",
      "epoch 3 | iter 1400\t | loss 644.2\n",
      "epoch 3 | iter 1450\t | loss 634.2\n",
      "epoch 3 | iter 1500\t | loss 621.5\n",
      "epoch 3 | iter 1550\t | loss 600.6\n",
      "epoch 3 | iter 1600\t | loss 592.6\n",
      "epoch 3 | iter 1650\t | loss 603.4\n",
      "epoch 3 | iter 1700\t | loss 619.1\n",
      "epoch 3 | iter 1750\t | loss 632.6\n",
      "epoch 4 | iter 0\t | loss 588.0\n",
      "epoch 4 | iter 50\t | loss 650.1\n",
      "epoch 4 | iter 100\t | loss 619.5\n",
      "epoch 4 | iter 150\t | loss 627.1\n",
      "epoch 4 | iter 200\t | loss 564.1\n",
      "epoch 4 | iter 250\t | loss 625.8\n",
      "epoch 4 | iter 300\t | loss 632.7\n",
      "epoch 4 | iter 350\t | loss 632.4\n",
      "epoch 4 | iter 400\t | loss 618.8\n",
      "epoch 4 | iter 450\t | loss 618.6\n",
      "epoch 4 | iter 500\t | loss 619.2\n",
      "epoch 4 | iter 550\t | loss 627.4\n",
      "epoch 4 | iter 600\t | loss 635.1\n",
      "epoch 4 | iter 650\t | loss 622.9\n",
      "epoch 4 | iter 700\t | loss 641.6\n",
      "epoch 4 | iter 750\t | loss 616.9\n",
      "epoch 4 | iter 800\t | loss 598.0\n",
      "epoch 4 | iter 850\t | loss 615.8\n",
      "epoch 4 | iter 900\t | loss 621.5\n",
      "epoch 4 | iter 950\t | loss 597.6\n",
      "epoch 4 | iter 1000\t | loss 627.0\n",
      "epoch 4 | iter 1050\t | loss 598.5\n",
      "epoch 4 | iter 1100\t | loss 602.7\n",
      "epoch 4 | iter 1150\t | loss 612.1\n",
      "epoch 4 | iter 1200\t | loss 619.4\n",
      "epoch 4 | iter 1250\t | loss 633.8\n",
      "epoch 4 | iter 1300\t | loss 598.4\n",
      "epoch 4 | iter 1350\t | loss 593.5\n",
      "epoch 4 | iter 1400\t | loss 571.8\n",
      "epoch 4 | iter 1450\t | loss 615.4\n",
      "epoch 4 | iter 1500\t | loss 639.8\n",
      "epoch 4 | iter 1550\t | loss 619.7\n",
      "epoch 4 | iter 1600\t | loss 610.9\n",
      "epoch 4 | iter 1650\t | loss 598.7\n",
      "epoch 4 | iter 1700\t | loss 622.7\n",
      "epoch 4 | iter 1750\t | loss 608.9\n",
      "mode: zsl\n",
      "DEVICE\n",
      "cuda:0\n",
      "self.input_dim\n",
      "64\n",
      "...\n",
      "[0.0]  acc=0.3394 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_240936/1767569103.py:445: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  history.append([0, torch.tensor(cls.acc).item(), 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0]  acc=0.3031 \n",
      "[2.0]  acc=0.2834 \n",
      "[3.0]  acc=0.2603 \n",
      "[4.0]  acc=0.2453 \n",
      "[5.0]  acc=0.2340 \n",
      "[6.0]  acc=0.2104 \n",
      "[7.0]  acc=0.2055 \n",
      "[8.0]  acc=0.1971 \n",
      "[9.0]  acc=0.1990 \n",
      "[10.0]  acc=0.1905 \n",
      "[11.0]  acc=0.1837 \n",
      "[12.0]  acc=0.1788 \n",
      "[13.0]  acc=0.1787 \n",
      "[14.0]  acc=0.1769 \n",
      "[15.0]  acc=0.1770 \n",
      "[16.0]  acc=0.1787 \n",
      "[17.0]  acc=0.1821 \n",
      "[18.0]  acc=0.1771 \n",
      "[19.0]  acc=0.1754 \n",
      "[20.0]  acc=0.1821 \n",
      "0.1820792853832245\n",
      ">> saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_240936/1767569103.py:451: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return 0, torch.tensor(cls.acc).item(), 0, history\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "########################################\n",
    "### load model where u left\n",
    "########################################\n",
    "saved_state = torch.load('./saved_models/CADA_trained.pth.tar')\n",
    "model.load_state_dict(saved_state['state_dict'])\n",
    "for d in model.all_data_sources_without_duplicates:\n",
    "    model.encoder[d].load_state_dict(saved_state['encoder'][d])\n",
    "    model.decoder[d].load_state_dict(saved_state['decoder'][d])\n",
    "########################################\n",
    "\"\"\"\n",
    "\n",
    "losses = model.train_vae()\n",
    "\n",
    "u,s,h,history = model.train_classifier()\n",
    "\n",
    "\n",
    "if hyperparameters['generalized']==True:\n",
    "    acc = [hi[2] for hi in history]\n",
    "elif hyperparameters['generalized']==False:\n",
    "    acc = [hi[1] for hi in history]\n",
    "\n",
    "print(acc[-1])\n",
    "\n",
    "\n",
    "state = {\n",
    "            'state_dict': model.state_dict() ,\n",
    "            'hyperparameters': hyperparameters,\n",
    "            'encoder': {},\n",
    "            'decoder': {},\n",
    "            'acc': acc,\n",
    "            'history': history,\n",
    "        }\n",
    "for d in model.all_data_sources:\n",
    "    state['encoder'][d] = model.encoder[d].state_dict()\n",
    "    state['decoder'][d] = model.decoder[d].state_dict()\n",
    "\n",
    "\n",
    "torch.save(state, 'CADA_trained.pth.tar')\n",
    "print('>> saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637dba2-8bdb-43b2-b742-984f05c5bf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
