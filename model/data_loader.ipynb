{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149ed80-b1e7-44d5-9f7c-d7249874fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "def map_label(label, classes):\n",
    "    mapped_label = torch.LongTensor(label.size())\n",
    "    for i in range(classes.size(0)):\n",
    "        mapped_label[label==classes[i]] = i\n",
    "\n",
    "    return mapped_label\n",
    "\n",
    "class DATA_LOADER(object):\n",
    "    def __init__(self, dataset, aux_datasource, device='cuda'):\n",
    "\n",
    "        print(\"The current working directory is\")\n",
    "        print(os.getcwd())\n",
    "        folder = str(Path(os.getcwd()))\n",
    "        if folder[-5:] == 'model':\n",
    "            project_directory = Path(os.getcwd()).parent\n",
    "        else:\n",
    "            project_directory = folder\n",
    "\n",
    "        print('Project Directory:')\n",
    "        print(project_directory)\n",
    "        data_path = str(project_directory) + '/data'\n",
    "        print('Data Path')\n",
    "        print(data_path)\n",
    "        sys.path.append(data_path)\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.device = device\n",
    "        self.dataset = dataset\n",
    "        self.auxiliary_data_source = aux_datasource\n",
    "\n",
    "        self.all_data_sources = ['resnet_features'] + [self.auxiliary_data_source]\n",
    "\n",
    "        if self.dataset == 'concept':\n",
    "            self.read_matdataset_concept()\n",
    "        else:\n",
    "            if self.dataset == 'CUB':\n",
    "                self.datadir = self.data_path + '/CUB/'\n",
    "            elif self.dataset == 'SUN':\n",
    "                self.datadir = self.data_path + '/SUN/'\n",
    "            elif self.dataset == 'AWA1':\n",
    "                self.datadir = self.data_path + '/AWA1/'\n",
    "            elif self.dataset == 'AWA2':\n",
    "                self.datadir = self.data_path + '/AWA2/'\n",
    "            self.read_matdataset()\n",
    "\n",
    "        self.index_in_epoch = 0\n",
    "        self.epochs_completed = 0\n",
    "\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        #####################################################################\n",
    "        # gets batch from train_feature = 7057 samples from 150 train classes\n",
    "        #####################################################################\n",
    "        idx = torch.randperm(self.ntrain)[0:batch_size]\n",
    "        batch_feature = self.data['train_seen']['resnet_features'][idx]\n",
    "        batch_label =  self.data['train_seen']['labels'][idx]\n",
    "        batch_att = self.aux_data[batch_label]\n",
    "        return batch_label, [ batch_feature, batch_att]\n",
    "\n",
    "\n",
    "    def read_matdataset_concept(self):\n",
    "\n",
    "        path= self.datadir + 'res101.mat'\n",
    "        print('_____')\n",
    "        print(path)\n",
    "        pdb.set_trace()\n",
    "        matcontent = sio.loadmat(path) # keys: 'image_files', 'features', 'labels']\n",
    "        feature = matcontent['features'].T  # [30475, 2048]\n",
    "        label = matcontent['labels'].astype(int).squeeze() - 1  # [30475]\n",
    "\n",
    "        path= self.datadir + 'att_splits.mat'\n",
    "        matcontent = sio.loadmat(path)\n",
    "        # numpy array index starts from 0, matlab starts from 1\n",
    "        trainval_loc = matcontent['trainval_loc'].squeeze() - 1  # (19832,)\n",
    "        train_loc = matcontent['train_loc'].squeeze() - 1 # (16864,) --> train_feature = TRAIN SEEN\n",
    "        val_unseen_loc = matcontent['val_loc'].squeeze() - 1 #(7926,)--> test_unseen_feature = TEST UNSEEN\n",
    "        test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1  # (4958,)\n",
    "        test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1  # (5685,)\n",
    "\n",
    "\n",
    "        if self.auxiliary_data_source == 'attributes':\n",
    "            self.aux_data = torch.from_numpy(matcontent['att'].T).float().to(self.device)  # [50, 85]\n",
    "        else:\n",
    "            if self.dataset != 'CUB':\n",
    "                print('the specified auxiliary datasource is not available for this dataset')\n",
    "            else:\n",
    "\n",
    "                with open(self.datadir + 'CUB_supporting_data.p', 'rb') as h:\n",
    "                    x = pickle.load(h)\n",
    "                    self.aux_data = torch.from_numpy(x[self.auxiliary_data_source]).float().to(self.device)\n",
    "\n",
    "\n",
    "                print('loaded ', self.auxiliary_data_source)\n",
    "\n",
    "\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        train_feature = scaler.fit_transform(feature[trainval_loc])  # (19832, 2048)\n",
    "        test_seen_feature = scaler.transform(feature[test_seen_loc])  # (4958, 2048)\n",
    "        test_unseen_feature = scaler.transform(feature[test_unseen_loc])  # (5685, 2048)\n",
    "\n",
    "        train_feature = torch.from_numpy(train_feature).float().to(self.device)\n",
    "        test_seen_feature = torch.from_numpy(test_seen_feature).float().to(self.device)\n",
    "        test_unseen_feature = torch.from_numpy(test_unseen_feature).float().to(self.device)\n",
    "\n",
    "        train_label = torch.from_numpy(label[trainval_loc]).long().to(self.device)\n",
    "        test_unseen_label = torch.from_numpy(label[test_unseen_loc]).long().to(self.device)\n",
    "        test_seen_label = torch.from_numpy(label[test_seen_loc]).long().to(self.device)\n",
    "\n",
    "        self.seenclasses = torch.from_numpy(np.unique(train_label.cpu().numpy())).to(self.device) # [40]\n",
    "        self.novelclasses = torch.from_numpy(np.unique(test_unseen_label.cpu().numpy())).to(self.device)  # [10]\n",
    "        self.ntrain = train_feature.size()[0]  # 19832\n",
    "        self.ntrain_class = self.seenclasses.size(0)  # 40\n",
    "        self.ntest_class = self.novelclasses.size(0)  # 10\n",
    "        self.train_class = self.seenclasses.clone()  # [40]\n",
    "        self.allclasses = torch.arange(0, self.ntrain_class+self.ntest_class).long()  #[0,...49]\n",
    "\n",
    "        self.train_mapped_label = map_label(train_label, self.seenclasses)  # [19832]\n",
    "\n",
    "        self.data = {}\n",
    "        self.data['train_seen'] = {}\n",
    "        self.data['train_seen']['resnet_features'] = train_feature\n",
    "        self.data['train_seen']['labels']= train_label\n",
    "        self.data['train_seen'][self.auxiliary_data_source] = self.aux_data[train_label]\n",
    "\n",
    "\n",
    "        self.data['train_unseen'] = {}\n",
    "        self.data['train_unseen']['resnet_features'] = None\n",
    "        self.data['train_unseen']['labels'] = None\n",
    "\n",
    "        self.data['test_seen'] = {}\n",
    "        self.data['test_seen']['resnet_features'] = test_seen_feature\n",
    "        self.data['test_seen']['labels'] = test_seen_label\n",
    "\n",
    "        self.data['test_unseen'] = {}\n",
    "        self.data['test_unseen']['resnet_features'] = test_unseen_feature\n",
    "        self.data['test_unseen'][self.auxiliary_data_source] = self.aux_data[test_unseen_label]\n",
    "        self.data['test_unseen']['labels'] = test_unseen_label\n",
    "\n",
    "        self.novelclass_aux_data = self.aux_data[self.novelclasses]\n",
    "        self.seenclass_aux_data = self.aux_data[self.seenclasses]\n",
    "\n",
    "\n",
    "    def read_matdataset(self):\n",
    "\n",
    "        path= self.datadir + 'res101.mat'\n",
    "        print('_____')\n",
    "        print(path)\n",
    "        pdb.set_trace()\n",
    "        matcontent = sio.loadmat(path) # keys: 'image_files', 'features', 'labels']\n",
    "        feature = matcontent['features'].T  # [30475, 2048]\n",
    "        label = matcontent['labels'].astype(int).squeeze() - 1  # [30475]\n",
    "\n",
    "        path= self.datadir + 'att_splits.mat'\n",
    "        matcontent = sio.loadmat(path)\n",
    "        # numpy array index starts from 0, matlab starts from 1\n",
    "        trainval_loc = matcontent['trainval_loc'].squeeze() - 1  # (19832,)\n",
    "        train_loc = matcontent['train_loc'].squeeze() - 1 # (16864,) --> train_feature = TRAIN SEEN\n",
    "        val_unseen_loc = matcontent['val_loc'].squeeze() - 1 #(7926,)--> test_unseen_feature = TEST UNSEEN\n",
    "        test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1  # (4958,)\n",
    "        test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1  # (5685,)\n",
    "\n",
    "\n",
    "        if self.auxiliary_data_source == 'attributes':\n",
    "            self.aux_data = torch.from_numpy(matcontent['att'].T).float().to(self.device)  # [50, 85]\n",
    "        else:\n",
    "            if self.dataset != 'CUB':\n",
    "                print('the specified auxiliary datasource is not available for this dataset')\n",
    "            else:\n",
    "\n",
    "                with open(self.datadir + 'CUB_supporting_data.p', 'rb') as h:\n",
    "                    x = pickle.load(h)\n",
    "                    self.aux_data = torch.from_numpy(x[self.auxiliary_data_source]).float().to(self.device)\n",
    "\n",
    "\n",
    "                print('loaded ', self.auxiliary_data_source)\n",
    "\n",
    "\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "        train_feature = scaler.fit_transform(feature[trainval_loc])  # (19832, 2048)\n",
    "        test_seen_feature = scaler.transform(feature[test_seen_loc])  # (4958, 2048)\n",
    "        test_unseen_feature = scaler.transform(feature[test_unseen_loc])  # (5685, 2048)\n",
    "\n",
    "        train_feature = torch.from_numpy(train_feature).float().to(self.device)\n",
    "        test_seen_feature = torch.from_numpy(test_seen_feature).float().to(self.device)\n",
    "        test_unseen_feature = torch.from_numpy(test_unseen_feature).float().to(self.device)\n",
    "\n",
    "        train_label = torch.from_numpy(label[trainval_loc]).long().to(self.device)\n",
    "        test_unseen_label = torch.from_numpy(label[test_unseen_loc]).long().to(self.device)\n",
    "        test_seen_label = torch.from_numpy(label[test_seen_loc]).long().to(self.device)\n",
    "\n",
    "        self.seenclasses = torch.from_numpy(np.unique(train_label.cpu().numpy())).to(self.device) # [40]\n",
    "        self.novelclasses = torch.from_numpy(np.unique(test_unseen_label.cpu().numpy())).to(self.device)  # [10]\n",
    "        self.ntrain = train_feature.size()[0]  # 19832\n",
    "        self.ntrain_class = self.seenclasses.size(0)  # 40\n",
    "        self.ntest_class = self.novelclasses.size(0)  # 10\n",
    "        self.train_class = self.seenclasses.clone()  # [40]\n",
    "        self.allclasses = torch.arange(0, self.ntrain_class+self.ntest_class).long()  #[0,...49]\n",
    "\n",
    "        self.train_mapped_label = map_label(train_label, self.seenclasses)  # [19832]\n",
    "\n",
    "        self.data = {}\n",
    "        self.data['train_seen'] = {}\n",
    "        self.data['train_seen']['resnet_features'] = train_feature\n",
    "        self.data['train_seen']['labels']= train_label\n",
    "        self.data['train_seen'][self.auxiliary_data_source] = self.aux_data[train_label]\n",
    "\n",
    "\n",
    "        self.data['train_unseen'] = {}\n",
    "        self.data['train_unseen']['resnet_features'] = None\n",
    "        self.data['train_unseen']['labels'] = None\n",
    "\n",
    "        self.data['test_seen'] = {}\n",
    "        self.data['test_seen']['resnet_features'] = test_seen_feature\n",
    "        self.data['test_seen']['labels'] = test_seen_label\n",
    "\n",
    "        self.data['test_unseen'] = {}\n",
    "        self.data['test_unseen']['resnet_features'] = test_unseen_feature\n",
    "        self.data['test_unseen'][self.auxiliary_data_source] = self.aux_data[test_unseen_label]\n",
    "        self.data['test_unseen']['labels'] = test_unseen_label\n",
    "\n",
    "        self.novelclass_aux_data = self.aux_data[self.novelclasses]\n",
    "        self.seenclass_aux_data = self.aux_data[self.seenclasses]\n",
    "\n",
    "\n",
    "    def transfer_features(self, n, num_queries='num_features'):\n",
    "        \"\"\"Only used for few-shot learning.\"\"\"\n",
    "        print('size before')\n",
    "        print(self.data['test_unseen']['resnet_features'].size())\n",
    "        print(self.data['train_seen']['resnet_features'].size())\n",
    "\n",
    "\n",
    "        print('o'*100)\n",
    "        print(self.data['test_unseen'].keys())\n",
    "        for i,s in enumerate(self.novelclasses):\n",
    "\n",
    "            features_of_that_class   = self.data['test_unseen']['resnet_features'][self.data['test_unseen']['labels']==s ,:]\n",
    "\n",
    "            if 'attributes' == self.auxiliary_data_source:\n",
    "                attributes_of_that_class = self.data['test_unseen']['attributes'][self.data['test_unseen']['labels']==s ,:]\n",
    "                use_att = True\n",
    "            else:\n",
    "                use_att = False\n",
    "            if 'sentences' == self.auxiliary_data_source:\n",
    "                sentences_of_that_class = self.data['test_unseen']['sentences'][self.data['test_unseen']['labels']==s ,:]\n",
    "                use_stc = True\n",
    "            else:\n",
    "                use_stc = False\n",
    "            if 'word2vec' == self.auxiliary_data_source:\n",
    "                word2vec_of_that_class = self.data['test_unseen']['word2vec'][self.data['test_unseen']['labels']==s ,:]\n",
    "                use_w2v = True\n",
    "            else:\n",
    "                use_w2v = False\n",
    "            if 'glove' == self.auxiliary_data_source:\n",
    "                glove_of_that_class = self.data['test_unseen']['glove'][self.data['test_unseen']['labels']==s ,:]\n",
    "                use_glo = True\n",
    "            else:\n",
    "                use_glo = False\n",
    "            if 'wordnet' == self.auxiliary_data_source:\n",
    "                wordnet_of_that_class = self.data['test_unseen']['wordnet'][self.data['test_unseen']['labels']==s ,:]\n",
    "                use_hie = True\n",
    "            else:\n",
    "                use_hie = False\n",
    "\n",
    "\n",
    "            num_features = features_of_that_class.size(0)\n",
    "\n",
    "            indices = torch.randperm(num_features)\n",
    "\n",
    "            if num_queries!='num_features':\n",
    "\n",
    "                indices = indices[:n+num_queries]\n",
    "\n",
    "\n",
    "            print(features_of_that_class.size())\n",
    "\n",
    "\n",
    "            if i==0:\n",
    "\n",
    "                new_train_unseen      = features_of_that_class[   indices[:n] ,:]\n",
    "\n",
    "                if use_att:\n",
    "                    new_train_unseen_att  = attributes_of_that_class[ indices[:n] ,:]\n",
    "                if use_stc:\n",
    "                    new_train_unseen_stc  = sentences_of_that_class[ indices[:n] ,:]\n",
    "                if use_w2v:\n",
    "                    new_train_unseen_w2v  = word2vec_of_that_class[ indices[:n] ,:]\n",
    "                if use_glo:\n",
    "                    new_train_unseen_glo  = glove_of_that_class[ indices[:n] ,:]\n",
    "                if use_hie:\n",
    "                    new_train_unseen_hie  = wordnet_of_that_class[ indices[:n] ,:]\n",
    "\n",
    "\n",
    "                new_train_unseen_label  = s.repeat(n)\n",
    "\n",
    "                new_test_unseen = features_of_that_class[  indices[n:] ,:]\n",
    "\n",
    "                new_test_unseen_label = s.repeat( len(indices[n:] ))\n",
    "\n",
    "            else:\n",
    "                new_train_unseen  = torch.cat(( new_train_unseen             , features_of_that_class[  indices[:n] ,:]),dim=0)\n",
    "                new_train_unseen_label  = torch.cat(( new_train_unseen_label , s.repeat(n)),dim=0)\n",
    "\n",
    "                new_test_unseen =  torch.cat(( new_test_unseen,    features_of_that_class[  indices[n:] ,:]),dim=0)\n",
    "                new_test_unseen_label = torch.cat(( new_test_unseen_label  ,s.repeat( len(indices[n:]) )) ,dim=0)\n",
    "\n",
    "                if use_att:\n",
    "                    new_train_unseen_att    = torch.cat(( new_train_unseen_att   , attributes_of_that_class[indices[:n] ,:]),dim=0)\n",
    "                if use_stc:\n",
    "                    new_train_unseen_stc    = torch.cat(( new_train_unseen_stc   , sentences_of_that_class[indices[:n] ,:]),dim=0)\n",
    "                if use_w2v:\n",
    "                    new_train_unseen_w2v    = torch.cat(( new_train_unseen_w2v   , word2vec_of_that_class[indices[:n] ,:]),dim=0)\n",
    "                if use_glo:\n",
    "                    new_train_unseen_glo    = torch.cat(( new_train_unseen_glo   , glove_of_that_class[indices[:n] ,:]),dim=0)\n",
    "                if use_hie:\n",
    "                    new_train_unseen_hie    = torch.cat(( new_train_unseen_hie   , wordnet_of_that_class[indices[:n] ,:]),dim=0)\n",
    "\n",
    "\n",
    "\n",
    "        print('new_test_unseen.size(): ', new_test_unseen.size())\n",
    "        print('new_test_unseen_label.size(): ', new_test_unseen_label.size())\n",
    "        print('new_train_unseen.size(): ', new_train_unseen.size())\n",
    "        #print('new_train_unseen_att.size(): ', new_train_unseen_att.size())\n",
    "        print('new_train_unseen_label.size(): ', new_train_unseen_label.size())\n",
    "        print('>> num novel classes: ' + str(len(self.novelclasses)))\n",
    "\n",
    "        #######\n",
    "        ##\n",
    "        #######\n",
    "\n",
    "        self.data['test_unseen']['resnet_features'] = copy.deepcopy(new_test_unseen)\n",
    "        #self.data['train_seen']['resnet_features']  = copy.deepcopy(new_train_seen)\n",
    "\n",
    "        self.data['test_unseen']['labels'] = copy.deepcopy(new_test_unseen_label)\n",
    "        #self.data['train_seen']['labels']  = copy.deepcopy(new_train_seen_label)\n",
    "\n",
    "        self.data['train_unseen']['resnet_features'] = copy.deepcopy(new_train_unseen)\n",
    "        self.data['train_unseen']['labels'] = copy.deepcopy(new_train_unseen_label)\n",
    "        self.ntrain_unseen = self.data['train_unseen']['resnet_features'].size(0)\n",
    "\n",
    "        if use_att:\n",
    "            self.data['train_unseen']['attributes'] = copy.deepcopy(new_train_unseen_att)\n",
    "        if use_w2v:\n",
    "            self.data['train_unseen']['word2vec']   = copy.deepcopy(new_train_unseen_w2v)\n",
    "        if use_stc:\n",
    "            self.data['train_unseen']['sentences']  = copy.deepcopy(new_train_unseen_stc)\n",
    "        if use_glo:\n",
    "            self.data['train_unseen']['glove']      = copy.deepcopy(new_train_unseen_glo)\n",
    "        if use_hie:\n",
    "            self.data['train_unseen']['wordnet']   = copy.deepcopy(new_train_unseen_hie)\n",
    "\n",
    "        ####\n",
    "        self.data['train_seen_unseen_mixed'] = {}\n",
    "        self.data['train_seen_unseen_mixed']['resnet_features'] = torch.cat((self.data['train_seen']['resnet_features'],self.data['train_unseen']['resnet_features']),dim=0)\n",
    "        self.data['train_seen_unseen_mixed']['labels'] = torch.cat((self.data['train_seen']['labels'],self.data['train_unseen']['labels']),dim=0)\n",
    "\n",
    "        self.ntrain_mixed = self.data['train_seen_unseen_mixed']['resnet_features'].size(0)\n",
    "\n",
    "        if use_att:\n",
    "            self.data['train_seen_unseen_mixed']['attributes'] = torch.cat((self.data['train_seen']['attributes'],self.data['train_unseen']['attributes']),dim=0)\n",
    "        if use_w2v:\n",
    "            self.data['train_seen_unseen_mixed']['word2vec'] = torch.cat((self.data['train_seen']['word2vec'],self.data['train_unseen']['word2vec']),dim=0)\n",
    "        if use_stc:\n",
    "            self.data['train_seen_unseen_mixed']['sentences'] = torch.cat((self.data['train_seen']['sentences'],self.data['train_unseen']['sentences']),dim=0)\n",
    "        if use_glo:\n",
    "            self.data['train_seen_unseen_mixed']['glove'] = torch.cat((self.data['train_seen']['glove'],self.data['train_unseen']['glove']),dim=0)\n",
    "        if use_hie:\n",
    "            self.data['train_seen_unseen_mixed']['wordnet'] = torch.cat((self.data['train_seen']['wordnet'],self.data['train_unseen']['wordnet']),dim=0)\n",
    "\n",
    "#d = DATA_LOADER()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
